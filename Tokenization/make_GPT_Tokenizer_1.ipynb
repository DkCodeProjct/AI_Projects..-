{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization :\n",
    "##  * Tokenization is at the heart of much weirdness of LLMs. Do not brush it off.\n",
    "\n",
    "   + Why can't LLM spell words? Tokenization.\n",
    "   \n",
    "   \n",
    "   + Why can't LLM do super simple string processing tasks like reversing a string? Tokenization.\n",
    "   \n",
    "   \n",
    "   + Why is LLM worse at non-English languages (e.g. Japanese)? Tokenization.\n",
    "   \n",
    "   \n",
    "   + Why is LLM bad at simple arithmetic? Tokenization.\n",
    "   \n",
    "   \n",
    "   + Why did GPT-2 have more than necessary trouble coding in Python? Tokenization.\n",
    "   \n",
    "   \n",
    "   + Why did my LLM abruptly halt when it sees the string \"<|endoftext|>\"? Tokenization.\n",
    "   \n",
    "   \n",
    "   + What is this weird warning I get about a \"trailing whitespace\"? Tokenization.\n",
    "   \n",
    "   \n",
    "   + Why the LLM break if I ask it about \"SolidGoldMagikarp\"? Tokenization.\n",
    "   \n",
    "   \n",
    "   + Why should I prefer to use YAML over JSON with LLMs? Tokenization.\n",
    "   \n",
    "   \n",
    "   + Why is LLM not actually end-to-end language modeling? Tokenization.\n",
    "   \n",
    "   \n",
    "   + What is the real root of suffering? Tokenization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----------------------------\n",
    "\n",
    "\n",
    "```python \n",
    "\n",
    " Tokenization is at the heart of much weirdness of LLMs. Do not brush it off.\n",
    " \n",
    " 127 + 677 = 804\n",
    " 1275 + 6773 = 8041\n",
    " \n",
    " Egg.\n",
    " I have an Egg.\n",
    " egg.\n",
    " EGG.\n",
    " \n",
    " ë§Œë‚˜ì„œ ë°˜ê°€ì›Œìš”. ì €ëŠ” OpenAIì—ì„œ ê°œë°œí•œ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì¸ ChatGPTìž…ë‹ˆë‹¤. ê¶ê¸ˆí•œ ê²ƒì´ ìžˆìœ¼ì‹œë©´ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”.\n",
    " \n",
    " for i in range(1, 101):\n",
    "     if i % 3 == 0 and i % 5 == 0:\n",
    "         print(\"FizzBuzz\")\n",
    "     elif i % 3 == 0:\n",
    "         print(\"Fizz\")\n",
    "     elif i % 5 == 0:\n",
    "         print(\"Buzz\")\n",
    "     else:\n",
    "         print(i)\n",
    "```\n",
    "\n",
    "# -----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### * So thsis is sentence with 3 diff, Str,:\n",
    "  * First its Korean, \n",
    "  * and then Specail Char, :Emoj ðŸ‘‹\n",
    "  * finaly English\n",
    "\n",
    "  + So now we need to feed this to TransF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ì•ˆë…•í•˜ì„¸ìš” ðŸ‘‹ (hello in Korean!)'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"ì•ˆë…•í•˜ì„¸ìš” ðŸ‘‹ (hello in Korean!)\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Str. in python Anyway..?\n",
    "\n",
    "  + in Py. Doc.;\n",
    "    * Textual data in Python is handled with str objects, or strings.,\n",
    "    \n",
    "    *  **Strings are immutable sequences of Unicode code points**. String literals are written in a variety of ways:\n",
    "\n",
    "\n",
    "# Unicodes :\n",
    "  \n",
    "   * Unicode, formally The **Unicode Standard**, \n",
    "   \n",
    "   * is a **text encoding standard** maintained by the Unicode Consortium designed to support the use of text in all of the **world's writing systems that can be digitized**.\n",
    "\n",
    "\n",
    "#### In python we can access, unicode of a str, Using **ord**  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50504,\n",
       " 45397,\n",
       " 54616,\n",
       " 49464,\n",
       " 50836,\n",
       " 32,\n",
       " 128075,\n",
       " 32,\n",
       " 40,\n",
       " 104,\n",
       " 101,\n",
       " 108,\n",
       " 108,\n",
       " 111,\n",
       " 32,\n",
       " 105,\n",
       " 110,\n",
       " 32,\n",
       " 75,\n",
       " 111,\n",
       " 114,\n",
       " 101,\n",
       " 97,\n",
       " 110,\n",
       " 33,\n",
       " 41]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access, all the unicode for Chars in sentence\n",
    "[ord(x) for x in 'ì•ˆë…•í•˜ì„¸ìš” ðŸ‘‹ (hello in Korean!)'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord('A')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UTF-8 ,/\n",
    "  \n",
    "  + UTF-8 is a **character encoding standard used for electronic communication**. \n",
    "  \n",
    "  + Defined by the Unicode Standard, the name is derived from **Unicode Transformation Format â€“ 8-bit**. \n",
    "  \n",
    "  + Almost every webpage is stored in UTF-8.\n",
    "\n",
    "\n",
    "### We Tend to use ``utf-8`` enc, Cos. Utf-16 and Utf-32, quite Wasting,\n",
    "\n",
    "### They have lot of 0 in enc, so in our case utf-8 is ok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[236,\n",
       " 149,\n",
       " 136,\n",
       " 235,\n",
       " 133,\n",
       " 149,\n",
       " 237,\n",
       " 149,\n",
       " 152,\n",
       " 236,\n",
       " 132,\n",
       " 184,\n",
       " 236,\n",
       " 154,\n",
       " 148,\n",
       " 32,\n",
       " 240,\n",
       " 159,\n",
       " 145,\n",
       " 139,\n",
       " 32,\n",
       " 40,\n",
       " 104,\n",
       " 101,\n",
       " 108,\n",
       " 108,\n",
       " 111,\n",
       " 32,\n",
       " 105,\n",
       " 110,\n",
       " 32,\n",
       " 75,\n",
       " 111,\n",
       " 114,\n",
       " 101,\n",
       " 97,\n",
       " 110,\n",
       " 33,\n",
       " 41]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Conver to utf-8\n",
    "list('ì•ˆë…•í•˜ì„¸ìš” ðŸ‘‹ (hello in Korean!)'.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But we cant just Feed this row Byte, input to a TransF..:\n",
    "\n",
    "  * So we have to Compres it using Byte Pair Encoding Algorithm\n",
    "\n",
    "# -----\n",
    "\n",
    "# Byte Pair Encoding\n",
    "  \n",
    "  + Byte pair encoding, (also known as digram coding) is an algorithm, first described in **1994** by **Philip Gage** \n",
    "  \n",
    "  + for **encoding strings of text into tabular** form for **use in downstream modeling**.\n",
    "\n",
    "  + Its modification is notable as the large LM tokenizer, with an ability to **combine both tokens that encode single characters (including single digits or single punctuation marks)** \n",
    "  \n",
    "  + and those that encode whole words (even the longest compound words).\n",
    "\n",
    "\n",
    "## Original algorithm.\n",
    "  \n",
    "  + The original algorithm operates by **iteratively replacing the most common contiguous sequences of characters** in a **target text** with unused ``'placeholder'`` bytes.\n",
    "\n",
    "   * Example:\n",
    "      + Suppose the data to be encoded is\n",
    "             \n",
    "                    aaabdaaabac\n",
    "      \n",
    "      + The byte pair ``\"aa\"`` occurs most often, **so it will be replaced by a byte that is not used in the data, such as \"Z\".** Now there is the following data and replacement table:\n",
    "\n",
    "                    ZabdZabac\n",
    "                    Z =aa\n",
    "\n",
    "\n",
    "### * So, we got, seq of only ``9`` token, but now with the voab of ``5``, Cos we got 5th vocab element ``Z``. to cat aa\n",
    "\n",
    "### * And we can continue Doing this,, by cat common chars. \n",
    "\n",
    "### * By cat, we ``decreas the token``, ``Increase the Vocab Siz``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======\n",
      "ï¼µï½Žï½‰ï½ƒï½ï½„ï½…! ðŸ…¤ðŸ…ðŸ…˜ðŸ…’ðŸ…žðŸ…“ðŸ…”â€½ ðŸ‡ºâ€ŒðŸ‡³â€ŒðŸ‡®â€ŒðŸ‡¨â€ŒðŸ‡´â€ŒðŸ‡©â€ŒðŸ‡ª! ðŸ˜„ The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to â€œsupport Unicodeâ€ in our software (whatever that meansâ€”like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I donâ€™t blame programmers for still finding the whole thing mysterious, even 30 years after Unicodeâ€™s inception.\n",
      "Len of Txt 533\n",
      "=======\n",
      "[239, 188, 181, 239, 189, 142, 239, 189, 137, 239, 189, 131, 239, 189, 143, 239, 189, 132, 239, 189, 133, 33, 32, 240, 159, 133, 164, 240, 159, 133, 157, 240, 159, 133, 152, 240, 159, 133, 146, 240, 159, 133, 158, 240, 159, 133, 147, 240, 159, 133, 148, 226, 128, 189, 32, 240, 159, 135, 186, 226, 128, 140, 240, 159, 135, 179, 226, 128, 140, 240, 159, 135, 174, 226, 128, 140, 240, 159, 135, 168, 226, 128, 140, 240, 159, 135, 180, 226, 128, 140, 240, 159, 135, 169, 226, 128, 140, 240, 159, 135, 170, 33, 32, 240, 159, 152, 132, 32, 84, 104, 101, 32, 118, 101, 114, 121, 32, 110, 97, 109, 101, 32, 115, 116, 114, 105, 107, 101, 115, 32, 102, 101, 97, 114, 32, 97, 110, 100, 32, 97, 119, 101, 32, 105, 110, 116, 111, 32, 116, 104, 101, 32, 104, 101, 97, 114, 116, 115, 32, 111, 102, 32, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 119, 111, 114, 108, 100, 119, 105, 100, 101, 46, 32, 87, 101, 32, 97, 108, 108, 32, 107, 110, 111, 119, 32, 119, 101, 32, 111, 117, 103, 104, 116, 32, 116, 111, 32, 226, 128, 156, 115, 117, 112, 112, 111, 114, 116, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 157, 32, 105, 110, 32, 111, 117, 114, 32, 115, 111, 102, 116, 119, 97, 114, 101, 32, 40, 119, 104, 97, 116, 101, 118, 101, 114, 32, 116, 104, 97, 116, 32, 109, 101, 97, 110, 115, 226, 128, 148, 108, 105, 107, 101, 32, 117, 115, 105, 110, 103, 32, 119, 99, 104, 97, 114, 95, 116, 32, 102, 111, 114, 32, 97, 108, 108, 32, 116, 104, 101, 32, 115, 116, 114, 105, 110, 103, 115, 44, 32, 114, 105, 103, 104, 116, 63, 41, 46, 32, 66, 117, 116, 32, 85, 110, 105, 99, 111, 100, 101, 32, 99, 97, 110, 32, 98, 101, 32, 97, 98, 115, 116, 114, 117, 115, 101, 44, 32, 97, 110, 100, 32, 100, 105, 118, 105, 110, 103, 32, 105, 110, 116, 111, 32, 116, 104, 101, 32, 116, 104, 111, 117, 115, 97, 110, 100, 45, 112, 97, 103, 101, 32, 85, 110, 105, 99, 111, 100, 101, 32, 83, 116, 97, 110, 100, 97, 114, 100, 32, 112, 108, 117, 115, 32, 105, 116, 115, 32, 100, 111, 122, 101, 110, 115, 32, 111, 102, 32, 115, 117, 112, 112, 108, 101, 109, 101, 110, 116, 97, 114, 121, 32, 97, 110, 110, 101, 120, 101, 115, 44, 32, 114, 101, 112, 111, 114, 116, 115, 44, 32, 97, 110, 100, 32, 110, 111, 116, 101, 115, 32, 99, 97, 110, 32, 98, 101, 32, 109, 111, 114, 101, 32, 116, 104, 97, 110, 32, 97, 32, 108, 105, 116, 116, 108, 101, 32, 105, 110, 116, 105, 109, 105, 100, 97, 116, 105, 110, 103, 46, 32, 73, 32, 100, 111, 110, 226, 128, 153, 116, 32, 98, 108, 97, 109, 101, 32, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 102, 111, 114, 32, 115, 116, 105, 108, 108, 32, 102, 105, 110, 100, 105, 110, 103, 32, 116, 104, 101, 32, 119, 104, 111, 108, 101, 32, 116, 104, 105, 110, 103, 32, 109, 121, 115, 116, 101, 114, 105, 111, 117, 115, 44, 32, 101, 118, 101, 110, 32, 51, 48, 32, 121, 101, 97, 114, 115, 32, 97, 102, 116, 101, 114, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 153, 115, 32, 105, 110, 99, 101, 112, 116, 105, 111, 110, 46]\n",
      "Len of Tken 616\n"
     ]
    }
   ],
   "source": [
    "txt = \"ï¼µï½Žï½‰ï½ƒï½ï½„ï½…! ðŸ…¤ðŸ…ðŸ…˜ðŸ…’ðŸ…žðŸ…“ðŸ…”â€½ ðŸ‡ºâ€ŒðŸ‡³â€ŒðŸ‡®â€ŒðŸ‡¨â€ŒðŸ‡´â€ŒðŸ‡©â€ŒðŸ‡ª! ðŸ˜„ The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to â€œsupport Unicodeâ€ in our software (whatever that meansâ€”like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I donâ€™t blame programmers for still finding the whole thing mysterious, even 30 years after Unicodeâ€™s inception.\"\n",
    "token = txt.encode('utf-8')\n",
    "token = list(map(int, token))\n",
    "\n",
    "print('=======')\n",
    "print(txt)\n",
    "print('Len of Txt',len(txt))\n",
    "\n",
    "\n",
    "print('=======')\n",
    "print(token)\n",
    "print('Len of Tken',len(token))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lest apply Byte Pair Encoding Algo. for this txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ===============================\n",
    "##### I implement some func to replace same chars but this isnt what we want.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'>' not supported between instances of 'int' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m counts:\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m maxCountPair \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcounts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcounts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Wrong: counts[maxCountPair] = 'ZZ'\u001b[39;00m\n\u001b[1;32m     18\u001b[0m symbol \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: '>' not supported between instances of 'int' and 'str'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "counts = {}\n",
    "for i in range(100):\n",
    "    for pair in zip(txt, txt[1:]):\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "        if counts is None:\n",
    "            mxCountPair = max(counts, key=counts.get)\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "txt = 'aaab ss s a a  dls s '\n",
    "couts = {}\n",
    "for i in range(100):\n",
    "    for pair in zip(txt, txt[1:]):\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    if not counts:\n",
    "        break\n",
    "    maxCountPair = max(counts, key=counts.get)\n",
    "    # Wrong: counts[maxCountPair] = 'ZZ'\n",
    "    symbol = f'X{i}'\n",
    "    txt = txt.replace(''.join(maxCountPair), symbol)\n",
    "    counts = {}\n",
    "\"\"\"\n",
    "#print(txt)\n",
    "#print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Text: ï¼µï½Žï½‰ï½ƒï½ï½„ï½…! ðŸ…¤ðŸ…ðŸ…˜ðŸ…’ðŸ…žðŸ…“ðŸ…”â€½ ðŸ‡ºâ€ŒðŸ‡³â€ŒðŸ‡®â€ŒðŸ‡¨â€ŒðŸ‡´â€ŒðŸ‡©â€ŒðŸ‡ª! ðŸ˜„ ThE9ADE91E97A6ADE91E97A6E9329A1A197E95ADE91E97A6E932ADE91E97A6ADE91E97A6E93X2very namE9ADE91E97A6ADE91E97A6E9329A1A197E95ADE91E97A6E932ADE91E97A6ADE91E97A6E93X2strikes fear and awE9ADE91E97A6ADE91E97A6E9329A1A197E95ADE91E97A6E932ADE91E97A6ADE91E97A6E93X2into thE9ADE91E97A6ADE91E97A6E9329A1A197E95ADE91E97A6E932ADE91E97A6ADE91E97A6E93X2hearts of programmers worldwide. WE9ADE91E97A6ADE91E97A6E9329A1A197E95ADE91E97A6E932ADE91E97A6ADE91E97A6E93X2all know wE9ADE91E97A6ADE91E97A6E9329A1A197E95ADE91E97A6E932ADE91E97A6ADE91E97A6E93X2ought to â€œsupport Unicodeâ€ in our softwarE9ADE91E97A6ADE91E97A6E9329A1A197E95ADE91E97A6E932ADE91E97A6ADE91E97A6E93X2(whatever that meansâ€”likE9ADE91E97A6ADE91E97A6E9329A1A197E95ADE91E97A6E932ADE91E97A6ADE91E97A6E93X2using wchar_t for all thE9ADE91E97A6ADE91E97A6E9329A1A197E95ADE91E97A6E932ADE91E97A6ADE91E97A6E93X2strings, right?). But UnicodE9ADE91E97A6ADE91E97A6E9329A1A197E95ADE91E97A6E932ADE91E97A6ADE91E97A6E93X2can bE9ADE91E97A6ADE91E97A6E9329A1A197E95ADE91E97A6E932ADE91E97A6ADE91E97A6E93X2abstruse, and diving into thE9ADE91E97A6ADE91E97A6E9329A1A197E95ADE91E97A6E932ADE91E97A6ADE91E97A6E93X2thousand-pagE9ADE91E97A6ADE91E97A6E9329A1A197E95ADE91E97A6E932ADE91E97A6ADE91E97A6E93X2UnicodE9ADE91E97A6ADE91E97A6E9329A1A197E95ADE91E97A6E932ADE91E97A6ADE91E97A6E93X2Standard plus its dozens of supplementary annexes, reports, and notes can bE9ADE91E97A6ADE91E97A6E9329A1A197E95ADE91E97A6E932ADE91E97A6ADE91E97A6E93X2morE9ADE91E97A6ADE91E97A6E9329A1A197E95ADE91E97A6E932ADE91E97A6ADE91E97A6E93X2than a littlE9ADE91E97A6ADE91E97A6E9329A1A197E95ADE91E97A6E932ADE91E97A6ADE91E97A6E93X2intimidating. I donâ€™t blamE9ADE91E97A6ADE91E97A6E9329A1A197E95ADE91E97A6E932ADE91E97A6ADE91E97A6E93X2programmers for still finding thE9ADE91E97A6ADE91E97A6E9329A1A197E95ADE91E97A6E932ADE91E97A6ADE91E97A6E93X2wholE9ADE91E97A6ADE91E97A6E9329A1A197E95ADE91E97A6E932ADE91E97A6ADE91E97A6E93X2thing mysterious, even 30 years after Unicodeâ€™s inception.\n",
      "Final Counts: {('ï¼µ', 'ï½Ž'): 1, ('ï½Ž', 'ï½‰'): 1, ('ï½‰', 'ï½ƒ'): 1, ('ï½ƒ', 'ï½'): 1, ('ï½', 'ï½„'): 1, ('ï½„', 'ï½…'): 1, ('ï½…', '!'): 1, ('!', ' '): 2, (' ', 'ðŸ…¤'): 1, ('ðŸ…¤', 'ðŸ…'): 1, ('ðŸ…', 'ðŸ…˜'): 1, ('ðŸ…˜', 'ðŸ…’'): 1, ('ðŸ…’', 'ðŸ…ž'): 1, ('ðŸ…ž', 'ðŸ…“'): 1, ('ðŸ…“', 'ðŸ…”'): 1, ('ðŸ…”', 'â€½'): 1, ('â€½', ' '): 1, (' ', 'ðŸ‡º'): 1, ('ðŸ‡º', '\\u200c'): 1, ('\\u200c', 'ðŸ‡³'): 1, ('ðŸ‡³', '\\u200c'): 1, ('\\u200c', 'ðŸ‡®'): 1, ('ðŸ‡®', '\\u200c'): 1, ('\\u200c', 'ðŸ‡¨'): 1, ('ðŸ‡¨', '\\u200c'): 1, ('\\u200c', 'ðŸ‡´'): 1, ('ðŸ‡´', '\\u200c'): 1, ('\\u200c', 'ðŸ‡©'): 1, ('ðŸ‡©', '\\u200c'): 1, ('\\u200c', 'ðŸ‡ª'): 1, ('ðŸ‡ª', '!'): 1, (' ', 'ðŸ˜„'): 1, ('ðŸ˜„', ' '): 1, (' ', 'T'): 1, ('T', 'h'): 1, ('h', 'X'): 5, ('X', '8'): 300, ('8', 'A'): 20, ('A', 'D'): 100, ('D', 'X'): 100, ('8', '1'): 100, ('1', 'X'): 100, ('8', '7'): 100, ('7', 'A'): 100, ('A', '6'): 100, ('6', 'A'): 40, ('6', 'X'): 60, ('8', '3'): 60, ('3', '2'): 40, ('2', '9'): 20, ('9', 'A'): 20, ('A', '1'): 40, ('1', 'A'): 20, ('1', '9'): 20, ('9', '7'): 20, ('7', 'X'): 20, ('8', '5'): 20, ('5', 'A'): 20, ('2', 'A'): 20, ('3', 'X'): 20, ('X', '2'): 20, ('2', 'v'): 1, ('v', 'e'): 3, ('e', 'r'): 6, ('r', 'y'): 2, ('y', ' '): 2, (' ', 'n'): 2, ('n', 'a'): 1, ('a', 'm'): 4, ('m', 'X'): 2, ('2', 's'): 2, ('s', 't'): 5, ('t', 'r'): 3, ('r', 'i'): 4, ('i', 'k'): 2, ('k', 'e'): 1, ('e', 's'): 3, ('s', ' '): 10, (' ', 'f'): 4, ('f', 'e'): 1, ('e', 'a'): 4, ('a', 'r'): 7, ('r', ' '): 6, (' ', 'a'): 8, ('a', 'n'): 10, ('n', 'd'): 6, ('d', ' '): 4, ('a', 'w'): 1, ('w', 'X'): 2, ('2', 'i'): 2, ('i', 'n'): 12, ('n', 't'): 4, ('t', 'o'): 3, ('o', ' '): 3, (' ', 't'): 6, ('t', 'h'): 8, ('2', 'h'): 1, ('h', 'e'): 1, ('r', 't'): 3, ('t', 's'): 3, (' ', 'o'): 3, ('o', 'f'): 3, ('f', ' '): 2, (' ', 'p'): 2, ('p', 'r'): 2, ('r', 'o'): 2, ('o', 'g'): 2, ('g', 'r'): 2, ('r', 'a'): 2, ('m', 'm'): 2, ('m', 'e'): 4, ('r', 's'): 3, (' ', 'w'): 3, ('w', 'o'): 1, ('o', 'r'): 6, ('r', 'l'): 1, ('l', 'd'): 1, ('d', 'w'): 1, ('w', 'i'): 1, ('i', 'd'): 2, ('d', 'e'): 3, ('e', '.'): 1, ('.', ' '): 3, (' ', 'W'): 1, ('W', 'X'): 1, ('2', 'a'): 2, ('a', 'l'): 2, ('l', 'l'): 3, ('l', ' '): 3, (' ', 'k'): 1, ('k', 'n'): 1, ('n', 'o'): 2, ('o', 'w'): 1, ('w', ' '): 1, ('2', 'o'): 1, ('o', 'u'): 4, ('u', 'g'): 1, ('g', 'h'): 2, ('h', 't'): 2, ('t', ' '): 6, (' ', 'â€œ'): 1, ('â€œ', 's'): 1, ('s', 'u'): 2, ('u', 'p'): 2, ('p', 'p'): 2, ('p', 'o'): 2, (' ', 'U'): 3, ('U', 'n'): 4, ('n', 'i'): 4, ('i', 'c'): 4, ('c', 'o'): 4, ('o', 'd'): 4, ('e', 'â€'): 1, ('â€', ' '): 1, (' ', 'i'): 4, ('n', ' '): 5, ('u', 'r'): 1, (' ', 's'): 3, ('s', 'o'): 1, ('f', 't'): 2, ('t', 'w'): 1, ('w', 'a'): 1, ('r', 'X'): 2, ('2', '('): 1, ('(', 'w'): 1, ('w', 'h'): 2, ('h', 'a'): 4, ('a', 't'): 3, ('t', 'e'): 4, ('e', 'v'): 2, (' ', 'm'): 2, ('n', 's'): 2, ('s', 'â€”'): 1, ('â€”', 'l'): 1, ('l', 'i'): 2, ('k', 'X'): 1, ('2', 'u'): 1, ('u', 's'): 5, ('s', 'i'): 1, ('n', 'g'): 6, ('g', ' '): 4, ('w', 'c'): 1, ('c', 'h'): 1, ('r', '_'): 1, ('_', 't'): 1, ('f', 'o'): 2, ('g', 's'): 1, ('s', ','): 4, (',', ' '): 5, (' ', 'r'): 2, ('i', 'g'): 1, ('t', '?'): 1, ('?', ')'): 1, (')', '.'): 1, (' ', 'B'): 1, ('B', 'u'): 1, ('u', 't'): 1, ('d', 'X'): 2, ('2', 'c'): 1, ('c', 'a'): 2, (' ', 'b'): 3, ('b', 'X'): 2, ('a', 'b'): 1, ('b', 's'): 1, ('r', 'u'): 1, ('s', 'e'): 1, ('e', ','): 1, (' ', 'd'): 3, ('d', 'i'): 2, ('i', 'v'): 1, ('v', 'i'): 1, ('2', 't'): 3, ('h', 'o'): 2, ('s', 'a'): 1, ('d', '-'): 1, ('-', 'p'): 1, ('p', 'a'): 1, ('a', 'g'): 1, ('g', 'X'): 1, ('2', 'U'): 1, ('2', 'S'): 1, ('S', 't'): 1, ('t', 'a'): 2, ('d', 'a'): 2, ('r', 'd'): 1, ('p', 'l'): 2, ('l', 'u'): 1, ('i', 't'): 2, ('d', 'o'): 2, ('o', 'z'): 1, ('z', 'e'): 1, ('e', 'n'): 3, ('l', 'e'): 1, ('e', 'm'): 1, ('n', 'n'): 1, ('n', 'e'): 1, ('e', 'x'): 1, ('x', 'e'): 1, ('r', 'e'): 1, ('e', 'p'): 2, ('o', 't'): 1, (' ', 'c'): 1, ('2', 'm'): 1, ('m', 'o'): 1, ('a', ' '): 1, (' ', 'l'): 1, ('t', 't'): 1, ('t', 'l'): 1, ('l', 'X'): 2, ('t', 'i'): 4, ('i', 'm'): 1, ('m', 'i'): 1, ('g', '.'): 1, (' ', 'I'): 1, ('I', ' '): 1, ('o', 'n'): 2, ('n', 'â€™'): 1, ('â€™', 't'): 1, ('b', 'l'): 1, ('l', 'a'): 1, ('2', 'p'): 1, ('i', 'l'): 1, ('f', 'i'): 1, ('2', 'w'): 1, ('o', 'l'): 1, ('h', 'i'): 1, ('m', 'y'): 1, ('y', 's'): 1, ('i', 'o'): 2, (' ', 'e'): 1, (' ', '3'): 1, ('3', '0'): 1, ('0', ' '): 1, (' ', 'y'): 1, ('y', 'e'): 1, ('a', 'f'): 1, ('e', 'â€™'): 1, ('â€™', 's'): 1, ('n', 'c'): 1, ('c', 'e'): 1, ('p', 't'): 1, ('n', '.'): 1}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "for i in range(10):\n",
    "    counts = {}\n",
    "    for pair in zip(txt, txt[1:]):\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    \n",
    "    if not counts:\n",
    "        break\n",
    "\n",
    "    maxCountPair = max(counts, key=counts.get) \n",
    "\n",
    "    a = ['E', 'W', 'X', 'A']\n",
    "    symbol = f'{random.choice(a)}{i}'\n",
    "\n",
    "    txt = txt.replace(''.join(maxCountPair), symbol)\n",
    "\n",
    "print(\"Final Text:\", txt)\n",
    "print(\"Final Counts:\", counts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ==============================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(20, (101, 32)), (15, (240, 159)), (12, (226, 128)), (12, (105, 110)), (10, (115, 32)), (10, (97, 110)), (10, (32, 97)), (9, (32, 116)), (8, (116, 104)), (7, (159, 135)), (7, (159, 133)), (7, (97, 114)), (6, (239, 189)), (6, (140, 240)), (6, (128, 140)), (6, (116, 32)), (6, (114, 32)), (6, (111, 114)), (6, (110, 103)), (6, (110, 100)), (6, (109, 101)), (6, (104, 101)), (6, (101, 114)), (6, (32, 105)), (5, (117, 115)), (5, (115, 116)), (5, (110, 32)), (5, (100, 101)), (5, (44, 32)), (5, (32, 115)), (4, (116, 105)), (4, (116, 101)), (4, (115, 44)), (4, (114, 105)), (4, (111, 117)), (4, (111, 100)), (4, (110, 116)), (4, (110, 105)), (4, (105, 99)), (4, (104, 97)), (4, (103, 32)), (4, (101, 97)), (4, (100, 32)), (4, (99, 111)), (4, (97, 109)), (4, (85, 110)), (4, (32, 119)), (4, (32, 111)), (4, (32, 102)), (4, (32, 85)), (3, (118, 101)), (3, (116, 115)), (3, (116, 114)), (3, (116, 111)), (3, (114, 116)), (3, (114, 115)), (3, (114, 101)), (3, (111, 102)), (3, (111, 32)), (3, (108, 108)), (3, (108, 101)), (3, (108, 32)), (3, (101, 115)), (3, (101, 110)), (3, (97, 116)), (3, (46, 32)), (3, (32, 240)), (3, (32, 112)), (3, (32, 109)), (3, (32, 100)), (3, (32, 98)), (2, (128, 153)), (2, (121, 32)), (2, (119, 104)), (2, (119, 101)), (2, (117, 112)), (2, (116, 97)), (2, (115, 117)), (2, (114, 121)), (2, (114, 111)), (2, (114, 97)), (2, (112, 114)), (2, (112, 112)), (2, (112, 111)), (2, (112, 108)), (2, (111, 110)), (2, (111, 103)), (2, (110, 115)), (2, (110, 111)), (2, (109, 109)), (2, (108, 105)), (2, (107, 101)), (2, (105, 116)), (2, (105, 111)), (2, (105, 107)), (2, (105, 100)), (2, (104, 116)), (2, (104, 111)), (2, (103, 114)), (2, (103, 104)), (2, (102, 116)), (2, (102, 111)), (2, (102, 32)), (2, (101, 226)), (2, (101, 118)), (2, (101, 112)), (2, (100, 111)), (2, (100, 105)), (2, (100, 97)), (2, (99, 97)), (2, (98, 101)), (2, (97, 108)), (2, (33, 32)), (2, (32, 114)), (2, (32, 110)), (2, (32, 99)), (1, (239, 188)), (1, (189, 143)), (1, (189, 142)), (1, (189, 137)), (1, (189, 133)), (1, (189, 132)), (1, (189, 131)), (1, (189, 32)), (1, (188, 181)), (1, (186, 226)), (1, (181, 239)), (1, (180, 226)), (1, (179, 226)), (1, (174, 226)), (1, (170, 33)), (1, (169, 226)), (1, (168, 226)), (1, (164, 240)), (1, (159, 152)), (1, (158, 240)), (1, (157, 240)), (1, (157, 32)), (1, (156, 115)), (1, (153, 116)), (1, (153, 115)), (1, (152, 240)), (1, (152, 132)), (1, (148, 226)), (1, (148, 108)), (1, (147, 240)), (1, (146, 240)), (1, (143, 239)), (1, (142, 239)), (1, (137, 239)), (1, (135, 186)), (1, (135, 180)), (1, (135, 179)), (1, (135, 174)), (1, (135, 170)), (1, (135, 169)), (1, (135, 168)), (1, (133, 164)), (1, (133, 158)), (1, (133, 157)), (1, (133, 152)), (1, (133, 148)), (1, (133, 147)), (1, (133, 146)), (1, (133, 33)), (1, (132, 239)), (1, (132, 32)), (1, (131, 239)), (1, (128, 189)), (1, (128, 157)), (1, (128, 156)), (1, (128, 148)), (1, (122, 101)), (1, (121, 115)), (1, (121, 101)), (1, (120, 101)), (1, (119, 111)), (1, (119, 105)), (1, (119, 99)), (1, (119, 97)), (1, (119, 32)), (1, (118, 105)), (1, (117, 116)), (1, (117, 114)), (1, (117, 103)), (1, (116, 119)), (1, (116, 116)), (1, (116, 108)), (1, (116, 63)), (1, (115, 226)), (1, (115, 111)), (1, (115, 105)), (1, (115, 101)), (1, (115, 97)), (1, (114, 117)), (1, (114, 108)), (1, (114, 100)), (1, (114, 95)), (1, (112, 116)), (1, (112, 97)), (1, (111, 122)), (1, (111, 119)), (1, (111, 116)), (1, (111, 108)), (1, (110, 226)), (1, (110, 110)), (1, (110, 101)), (1, (110, 99)), (1, (110, 97)), (1, (110, 46)), (1, (109, 121)), (1, (109, 111)), (1, (109, 105)), (1, (108, 117)), (1, (108, 100)), (1, (108, 97)), (1, (107, 110)), (1, (105, 118)), (1, (105, 109)), (1, (105, 108)), (1, (105, 103)), (1, (104, 105)), (1, (103, 115)), (1, (103, 101)), (1, (103, 46)), (1, (102, 105)), (1, (102, 101)), (1, (101, 120)), (1, (101, 109)), (1, (101, 46)), (1, (101, 44)), (1, (100, 119)), (1, (100, 45)), (1, (99, 104)), (1, (99, 101)), (1, (98, 115)), (1, (98, 108)), (1, (97, 119)), (1, (97, 103)), (1, (97, 102)), (1, (97, 98)), (1, (97, 32)), (1, (95, 116)), (1, (87, 101)), (1, (84, 104)), (1, (83, 116)), (1, (73, 32)), (1, (66, 117)), (1, (63, 41)), (1, (51, 48)), (1, (48, 32)), (1, (45, 112)), (1, (41, 46)), (1, (40, 119)), (1, (32, 226)), (1, (32, 121)), (1, (32, 118)), (1, (32, 117)), (1, (32, 108)), (1, (32, 107)), (1, (32, 104)), (1, (32, 101)), (1, (32, 87)), (1, (32, 84)), (1, (32, 83)), (1, (32, 73)), (1, (32, 66)), (1, (32, 51)), (1, (32, 40))]\n"
     ]
    }
   ],
   "source": [
    "def getStats(ids):\n",
    "    counts = {}\n",
    "    for pair in zip(ids, ids[1:]):\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "stats = getStats(token)\n",
    "print(sorted(((v, k) for k, v in stats.items()), reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ord() give you unicode Val -- > ord(a)-> 92 \n",
    "# chr() give you char of unicode val -- > chr(102)->'a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('e', ' ')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(101), chr(32) # So e and space is the most Occuring chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101, 32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxOcuringPair = max(stats, key=stats.get)\n",
    "maxOcuringPair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 99, 7, 9, 1]\n"
     ]
    }
   ],
   "source": [
    "def merge(ids, pair, ix):\n",
    "    newId = []\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        # if we are not at the very last Position and The pair matches, Replace it \n",
    "        if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
    "            newId.append(ix)\n",
    "            i += 2\n",
    "        else:\n",
    "            newId.append(ids[i])\n",
    "            i += 1\n",
    "    return newId\n",
    "\n",
    "print(merge([5, 6, 6, 7, 9, 1], (6, 6), 99))\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[239, 188, 181, 239, 189, 142, 239, 189, 137, 239, 189, 131, 239, 189, 143, 239, 189, 132, 239, 189, 133, 33, 32, 240, 159, 133, 164, 240, 159, 133, 157, 240, 159, 133, 152, 240, 159, 133, 146, 240, 159, 133, 158, 240, 159, 133, 147, 240, 159, 133, 148, 226, 128, 189, 32, 240, 159, 135, 186, 226, 128, 140, 240, 159, 135, 179, 226, 128, 140, 240, 159, 135, 174, 226, 128, 140, 240, 159, 135, 168, 226, 128, 140, 240, 159, 135, 180, 226, 128, 140, 240, 159, 135, 169, 226, 128, 140, 240, 159, 135, 170, 33, 32, 240, 159, 152, 132, 32, 84, 104, 256, 118, 101, 114, 121, 32, 110, 97, 109, 256, 115, 116, 114, 105, 107, 101, 115, 32, 102, 101, 97, 114, 32, 97, 110, 100, 32, 97, 119, 256, 105, 110, 116, 111, 32, 116, 104, 256, 104, 101, 97, 114, 116, 115, 32, 111, 102, 32, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 119, 111, 114, 108, 100, 119, 105, 100, 101, 46, 32, 87, 256, 97, 108, 108, 32, 107, 110, 111, 119, 32, 119, 256, 111, 117, 103, 104, 116, 32, 116, 111, 32, 226, 128, 156, 115, 117, 112, 112, 111, 114, 116, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 157, 32, 105, 110, 32, 111, 117, 114, 32, 115, 111, 102, 116, 119, 97, 114, 256, 40, 119, 104, 97, 116, 101, 118, 101, 114, 32, 116, 104, 97, 116, 32, 109, 101, 97, 110, 115, 226, 128, 148, 108, 105, 107, 256, 117, 115, 105, 110, 103, 32, 119, 99, 104, 97, 114, 95, 116, 32, 102, 111, 114, 32, 97, 108, 108, 32, 116, 104, 256, 115, 116, 114, 105, 110, 103, 115, 44, 32, 114, 105, 103, 104, 116, 63, 41, 46, 32, 66, 117, 116, 32, 85, 110, 105, 99, 111, 100, 256, 99, 97, 110, 32, 98, 256, 97, 98, 115, 116, 114, 117, 115, 101, 44, 32, 97, 110, 100, 32, 100, 105, 118, 105, 110, 103, 32, 105, 110, 116, 111, 32, 116, 104, 256, 116, 104, 111, 117, 115, 97, 110, 100, 45, 112, 97, 103, 256, 85, 110, 105, 99, 111, 100, 256, 83, 116, 97, 110, 100, 97, 114, 100, 32, 112, 108, 117, 115, 32, 105, 116, 115, 32, 100, 111, 122, 101, 110, 115, 32, 111, 102, 32, 115, 117, 112, 112, 108, 101, 109, 101, 110, 116, 97, 114, 121, 32, 97, 110, 110, 101, 120, 101, 115, 44, 32, 114, 101, 112, 111, 114, 116, 115, 44, 32, 97, 110, 100, 32, 110, 111, 116, 101, 115, 32, 99, 97, 110, 32, 98, 256, 109, 111, 114, 256, 116, 104, 97, 110, 32, 97, 32, 108, 105, 116, 116, 108, 256, 105, 110, 116, 105, 109, 105, 100, 97, 116, 105, 110, 103, 46, 32, 73, 32, 100, 111, 110, 226, 128, 153, 116, 32, 98, 108, 97, 109, 256, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 102, 111, 114, 32, 115, 116, 105, 108, 108, 32, 102, 105, 110, 100, 105, 110, 103, 32, 116, 104, 256, 119, 104, 111, 108, 256, 116, 104, 105, 110, 103, 32, 109, 121, 115, 116, 101, 114, 105, 111, 117, 115, 44, 32, 101, 118, 101, 110, 32, 51, 48, 32, 121, 101, 97, 114, 115, 32, 97, 102, 116, 101, 114, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 153, 115, 32, 105, 110, 99, 101, 112, 116, 105, 111, 110, 46]\n",
      "596\n"
     ]
    }
   ],
   "source": [
    "mergToken = merge(token, maxOcuringPair, 256)\n",
    "print(mergToken)\n",
    "print(len(mergToken))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mergin para: (101, 32), to --> 256\n",
      "mergin para: (240, 159), to --> 257\n",
      "mergin para: (226, 128), to --> 258\n",
      "mergin para: (105, 110), to --> 259\n",
      "mergin para: (115, 32), to --> 260\n",
      "mergin para: (97, 110), to --> 261\n",
      "mergin para: (116, 104), to --> 262\n",
      "mergin para: (257, 133), to --> 263\n",
      "mergin para: (257, 135), to --> 264\n",
      "mergin para: (97, 114), to --> 265\n",
      "mergin para: (239, 189), to --> 266\n",
      "mergin para: (258, 140), to --> 267\n",
      "mergin para: (267, 264), to --> 268\n",
      "mergin para: (101, 114), to --> 269\n",
      "mergin para: (111, 114), to --> 270\n",
      "mergin para: (116, 32), to --> 271\n",
      "mergin para: (259, 103), to --> 272\n",
      "mergin para: (115, 116), to --> 273\n",
      "mergin para: (261, 100), to --> 274\n",
      "mergin para: (32, 262), to --> 275\n"
     ]
    }
   ],
   "source": [
    "vocabSiz = 276\n",
    "numMergs = vocabSiz - 256\n",
    "ids = list(token) # save a copy of original list\n",
    "\n",
    "mergs = {}\n",
    "for i in range(numMergs):\n",
    "    stats = getStats(ids)\n",
    "    pair = max(stats, key=stats.get)\n",
    "    ix = 256 + i\n",
    "    print(f'mergin para: {pair}, to --> {ix}')\n",
    "    ids = merge(ids, pair, ix)\n",
    "    mergs[pair] = ix \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenLen  616\n",
      "ids len  451\n",
      "compression raitio 1.37X\n"
     ]
    }
   ],
   "source": [
    "print('tokenLen ', len(token))\n",
    "print('ids len ', len(ids))\n",
    "print(f'compression raitio {len(token) / len(ids):.2f}X',)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, the Tokenizer is a completely separate, independent module from the LLM. It has its own training dataset of text (which could be different from that of the LLM), on which you train the vocabulary using the Byte Pair Encoding (BPE) algorithm. It then translates back and forth between raw text and sequences of tokens. The LLM later only ever sees the tokens and never directly deals with any text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoding \n",
    " \n",
    "  + given a seq of intgers in the range of [0, vocabSiz], Whats next?>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token id to byte Object For that token\n",
    "vocab = {ix:bytes([ix]) for ix in range(256)} \n",
    "for (p0, p1), ix in mergs.items():\n",
    "    vocab[ix] = vocab[p0] + vocab[p1] \n",
    "\n",
    "\n",
    "def decod(ids):\n",
    "    # given ids (list of int), return python str /\n",
    "    tokens = b\"\".join(vocab[ix] for ix in ids)\n",
    "    txt = tokens.decode('utf-8', errors='replace')   \n",
    "    return txt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So if we didnt use this [[error=replace]], we could get unicodeDecodeError,\n",
    " \n",
    "  + Cos This 128 isn't a standard Unicode Repr.\n",
    "\n",
    "  + **Read More On Docs, About Eror Handling** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ï¿½\n"
     ]
    }
   ],
   "source": [
    "print(decod([128]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding \n",
    " \n",
    "  + given a str, return list of int ( the tokens )\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[104, 101, 108, 108, 111, 32, 119, 270, 108, 100]\n"
     ]
    }
   ],
   "source": [
    "def enc(txt):\n",
    "    tokn = list(txt.encode('utf-8'))\n",
    "    while len(tokn) >= 2:\n",
    "        stats = getStats(tokn)\n",
    "        ## if not pair that didnt emegi, assing it to inf\n",
    "        pair = min(stats, key=lambda p: mergs.get(p, float('inf')))\n",
    "        if pair not in mergs:\n",
    "            break\n",
    "        ix = mergs[pair]\n",
    "        tokn = merge(tokn, pair, ix)\n",
    "    return tokn      \n",
    "print(enc('hello world'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello words\n"
     ]
    }
   ],
   "source": [
    "print(decod(enc('hello words')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "txt2 = decod(enc(txt))\n",
    "print(txt2 == txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "valTxt = 'Your code has the right idea of using a mapping (itos) to decode IDs into characters, but there are some issues in your implementation. Hereâ€™s the explanation and fixes:'\n",
    "valTxt2 = decod(enc(valTxt))\n",
    "print(valTxt == valTxt2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forced splits Using regex Patterns (GPT series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE:\n",
    "  \n",
    "  + **\\p{L} or \\p{Letter}**: any kind of letter from any language\n",
    "\n",
    "  \n",
    "  + **\\p{N} or \\p{Number}**: any kind of numeric character in any script.\n",
    "\n",
    "  \n",
    "  + Patterns Explained:\n",
    "      + s matches the character s.\n",
    "        \n",
    "\n",
    "      + ``'t `` matches the contraction ``n't (as in \"can't\" or \"don't\").``\n",
    "        \n",
    "\n",
    "        \n",
    "      + ``'re``  matches the contraction ``'re (as in \"you're\").``\n",
    "        \n",
    "\n",
    "        \n",
    "      + ``'ve``  matches the contraction ``'ve (as in \"we've\").``\n",
    "        \n",
    "\n",
    "        \n",
    "      + ``'m `` matches the contraction ``'m (as in \"I'm\").``\n",
    "        \n",
    "\n",
    "\n",
    "      + ``'ll``  matches the contraction ``'ll (as in \"you'll\").``\n",
    "        \n",
    "        \n",
    "\n",
    "      + ``'d `` matches the contraction ``'d (as in \"he'd\" or \"she'd\").``\n",
    "\n",
    "\n",
    "\n",
    "      + ``?[^\\s\\p{L}\\p{N}]+`` matches the constraction, ``Not Letters of Num. [Punctuation]``\n",
    "\n",
    "\n",
    "      + ``\\s+(?!\\S)|`` matches the constraction, ``Whitespace, '    hi', Up to last so--> '  ', ' hi'``   \n",
    "\n",
    "\n",
    "\n",
    "      + ``\\s+`` mathes the constraciton , ``catch Whitespce``\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re \n",
    "\n",
    "gpt2Pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', \"'ve\"]\n",
      "['hellow', ' worlds']\n",
      "['helo', '123', ' worlds']\n",
      "['how', '`', 's', ' life']\n",
      "['helo', '.???!!']\n",
      "['           ', ' helo']\n"
     ]
    }
   ],
   "source": [
    "print(re.findall(gpt2Pat,  \"hello've\"))# |'ve\n",
    "\n",
    "print(re.findall(gpt2Pat, 'hellow worlds')) # \\p{L}\n",
    "\n",
    "print(re.findall(gpt2Pat, 'helo123 worlds')) #\\p{N}\n",
    "\n",
    "print(re.findall(gpt2Pat, 'how`s life')) # hard code for this ['], \n",
    "\n",
    "print(re.findall(gpt2Pat, 'helo.???!!')) # ?[^\\s\\p{L}\\p{N}]+ ,,, Not letters or num.,, Catch Punctuation\n",
    "\n",
    "print(re.findall(gpt2Pat, '            helo')) # \\s+(?!\\S)|\\s+ ,, Whistapce\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', 'for', ' i', ' in', ' range', '(', '1', ',', ' 101', '):', '\\n   ', ' if', ' i', ' %', ' 3', ' ==', ' 0', ' or', ' i', ' %', ' 5', ' ==', ' 0', ':', '\\n       ', ' print', '(\"', 'FizzBuzz', '\")', '\\n   ', ' elif', ' i', ' %', ' 3', ' ==', ' 0', ':', '\\n       ', ' print', \"('\", 'Fizz', \"')\", '\\n   ', ' elif', ' i', ' %', ' 5', ' ==', ' 0', ':', '\\n       ', ' print', \"('\", 'Buzz', \"')\", '\\n   ', ' else', ':', '\\n       ', ' print', '(', 'i', ')', '\\n']\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "example = \"\"\"\n",
    "for i in range(1, 101):\n",
    "    if i % 3 == 0 or i % 5 == 0:\n",
    "        print(\"FizzBuzz\")\n",
    "    elif i % 3 == 0:\n",
    "        print('Fizz')\n",
    "    elif i % 5 == 0:\n",
    "        print('Buzz')\n",
    "    else:\n",
    "        print(i)\n",
    "\"\"\"\n",
    "\n",
    "print(re.findall(gpt2Pat, example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[220, 220, 23748, 995, 10185]\n",
      "[256, 24748, 1917, 12340]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "# GPT-2, Dose not Merge ``Spaces``\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "print(enc.encode(\"   hello world!!!\"))\n",
    "\n",
    "\n",
    "# GPT-4, Merge Spaces\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "print(enc.encode(\"   hello world!!!\"))\n",
    "\n",
    "# get Visual Understand in Tiktokenizwe website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reference the GPT-2 Enc, Download the vocab.bpe and encoder.json, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-11-24 15:50:20--  https://openaipublic.blob.core.windows.net/gpt-2/models/1558M/vocab.bpe\n",
      "Resolving openaipublic.blob.core.windows.net (openaipublic.blob.core.windows.net)... 57.150.97.129\n",
      "Connecting to openaipublic.blob.core.windows.net (openaipublic.blob.core.windows.net)|57.150.97.129|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 456318 (446K) [application/octet-stream]\n",
      "Saving to: â€˜vocab.bpeâ€™\n",
      "\n",
      "vocab.bpe           100%[===================>] 445.62K   344KB/s    in 1.3s    \n",
      "\n",
      "2024-11-24 15:50:23 (344 KB/s) - â€˜vocab.bpeâ€™ saved [456318/456318]\n",
      "\n",
      "--2024-11-24 15:50:23--  https://openaipublic.blob.core.windows.net/gpt-2/models/1558M/encoder.json\n",
      "Resolving openaipublic.blob.core.windows.net (openaipublic.blob.core.windows.net)... 57.150.97.129\n",
      "Connecting to openaipublic.blob.core.windows.net (openaipublic.blob.core.windows.net)|57.150.97.129|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1042301 (1018K) [application/json]\n",
      "Saving to: â€˜encoder.jsonâ€™\n",
      "\n",
      "encoder.json        100%[===================>]   1018K   474KB/s    in 2.1s    \n",
      "\n",
      "2024-11-24 15:50:27 (474 KB/s) - â€˜encoder.jsonâ€™ saved [1042301/1042301]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://openaipublic.blob.core.windows.net/gpt-2/models/1558M/vocab.bpe\n",
    "!wget https://openaipublic.blob.core.windows.net/gpt-2/models/1558M/encoder.json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json \n",
    "\n",
    "with open('encoder.json', 'r') as file:\n",
    "    enc = json.load(file)\n",
    "\n",
    "with open('vocab.bpe', 'r', encoding='utf-8') as file:\n",
    "    bpeData = file.read()\n",
    "\n",
    "bpeMerges = [tuple(mergStr.split()) for mergStr in bpeData.split('\\n')[1:-1] ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Special Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(enc) # 256 raw byte tokens + 50,000 merges +1 special Token 50,257"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50256"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what is 57 token\n",
    "enc['<|endoftext|>'] # 57 token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
