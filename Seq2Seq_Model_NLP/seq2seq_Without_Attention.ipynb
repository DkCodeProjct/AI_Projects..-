{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **NLP From Scratch**: Translation with a Sequence to Sequence Network and Attention\n",
    "  + In this project we will be teaching a neural network to translate from French to English.\n",
    "\n",
    "    -  [KEY: > input, = target, < output]\n",
    "      \n",
    "       ####  > il est en train de peindre un tableau .  \n",
    "       ####  = he is painting a picture .\n",
    "       ####  < he is painting a picture . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import  unicode_literals, print_function, division\n",
    "\n",
    "from io import open\n",
    "import unicodedata\n",
    "import re\n",
    "import random \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edit with Google Colab,, put images and explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = 0 #start of sentence\n",
    "E = 1 #end of sentnce\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, langName):\n",
    "        self.langName = langName\n",
    "        self.wtoi = {} #word to indx\n",
    "        self.wordToCount = {}\n",
    "        self.itow = {0:'<S>', 1:'<E'} # indx to word\n",
    "        self.nwords = 2\n",
    "    \n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "    \n",
    "    def addWord(self, word):\n",
    "        if word not in self.wtoi:\n",
    "            self.wtoi[word] = self.nwords\n",
    "            self.wordToCount[word] = 1\n",
    "            self.itow[self.nwords] = word\n",
    "            self.nwords += 1  # Count S and E\n",
    "        \n",
    "        else:\n",
    "            self.wordToCount[word] += 1\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The files are all in Unicode, to simplify we will turn Unicode characters to ASCII, make everything lowercase, and trim most punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# https://stackoverflow.com/a/518232/2809427\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "\n",
    "def normalizStr(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r'([.!?])', r\" \\1\", s)\n",
    "    s = re.sub(r'[^a-zA-Z!?]+', r' ', s)\n",
    "    return s.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To read the data file we will split the file into lines, and then split lines into pairs. The files are all English → Other Language, so if we want to translate from Other Language → English I added the reverse flag to reverse the pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def loadData(filePath, reverse=False):\n",
    "    pairs = []\n",
    "    with open(filePath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            eng, fre = line.strip().split('\\t') #english french\n",
    "\n",
    "            if reverse:\n",
    "                pairs.append([fre, eng])\n",
    "            \n",
    "            else:\n",
    "                pairs.append([eng, fre])\n",
    "\n",
    "    return pairs    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Since there are a lot of example sentences and we want to train something quickly, we’ll trim the data set to only relatively short and simple sentences. Here the maximum length is 10 words (that includes ending punctuation) and we’re filtering to sentences that translate to the form “I am” or “He is” etc. (accounting for apostrophes replaced earlier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 10\n",
    "\n",
    "engPrefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "def filtrPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LEN and len(p[1].split(' ')) < MAX_LEN and p[1].startswith(engPrefixes)\n",
    "\n",
    "def filtrPairs(pairs):\n",
    "    return [pair for pair in pairs if filtrPair(pair)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The full process for preparing the data is:\n",
    "   \n",
    "   + Read text file and split into lines, split lines into pairs\n",
    "\n",
    "   + Normalize text, filter by length and content\n",
    "\n",
    "   + Make word lists from sentences in pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizStr(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))  # Check if pairs were loaded correctly\n",
    "\n",
    "    pairs = filtrPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs after filtering\" % len(pairs))  # Check after filtering\n",
    "    print(\"Counting words...\")\n",
    "    \n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    \n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.langName, input_lang.nwords)\n",
    "    print(output_lang.langName, output_lang.nwords)\n",
    "    \n",
    "    if not pairs:\n",
    "        print(\"Warning: No pairs found after filtering. Check data or filtering criteria.\")\n",
    "    \n",
    "    # The rest of your code for counting words etc.\n",
    "    return input_lang, output_lang, pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 135842 sentence pairs\n",
      "Trimmed to 11445 sentence pairs after filtering\n",
      "Counting words...\n",
      "Counted words:\n",
      "fra 4601\n",
      "eng 2991\n",
      "['je pense a mes vacances', 'i am thinking of my vacation']\n"
     ]
    }
   ],
   "source": [
    "inputLang, outputLang, pairs = prepareData('eng', 'fra', reverse=True)\n",
    "print(random.choice(pairs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Encoder\n",
    "   + The encoder of a seq2seq network is a RNN that outputs some value for every word from the input sentence. \n",
    "   \n",
    "   + For every input word the encoder outputs a vector and a hidden state, and uses the hidden state for the next input word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, nInput, nHidden, dropoutP=0.1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.nHidden = nHidden\n",
    "\n",
    "        self.emb = nn.Embedding(nInput, nHidden)\n",
    "        self.gru = nn.GRU(nHidden, nHidden, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropoutP)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        emb = self.dropout(self.emb(x))\n",
    "        x, hidden = self.gru(emb)\n",
    "        return x, hidden\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Decoder\n",
    "   + The decoder is another RNN that takes the encoder output vector(s) and outputs a sequence of words to create the translation.\n",
    "\n",
    "## Simple Decoder\n",
    "   + In the simplest seq2seq decoder we use only last output of the encoder. \n",
    "    \n",
    "   + This last output is sometimes called the context vector as it encodes context from the entire sequence. \n",
    "   \n",
    "   + This context vector is used as the initial hidden state of the decoder.\n",
    "\n",
    "  * At every step of decoding, the decoder is given an input token and hidden state. \n",
    "  \n",
    "  * The initial input token is the start-of-string <SOS> token, and the first hidden state is the context vector (the encoder’s last hidden state)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, nHidden, nOutput):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.emb = nn.Embedding(nOutput, nHidden)\n",
    "        self.gru = nn.GRU(nHidden, nHidden, batch_first=True)\n",
    "        self.out = nn.Linear(nHidden, nOutput)\n",
    "    \n",
    "    def forward(self, encOutput, encHidden, targtTensor=None):\n",
    "        batchSiz = encOutput.size(0)\n",
    "        decodInput = torch.empty(batchSiz, 1, dtype=torch.long, device=device).fill_(S)\n",
    "        \n",
    "        decodHidden = encHidden\n",
    "        decodOutputs = []\n",
    "\n",
    "        for i in range(MAX_LEN):\n",
    "            decodOutput, decodHidden = self.forwardStep(decodInput, decodHidden)\n",
    "            decodOutputs.append(decodOutput)\n",
    "\n",
    "            if targtTensor is not None:\n",
    "                decodInput = targtTensor[:, i].unsqueeze(1)\n",
    "            \n",
    "            else:\n",
    "                _, topi = decodOutput.topk(i)\n",
    "                decodInput = topi.squeeze(-1).detach()\n",
    "        \n",
    "        decodOutputs = torch.cat(decodOutputs, dim=1)\n",
    "        decodOutputs = F.log_softmax(decodOutputs, dim=1)\n",
    "        \n",
    "        return decodOutputs, decodHidden, None \n",
    "\n",
    "    def forwardStep(self, x, hidden):\n",
    "        x = self.emb(x)\n",
    "        x = F.relu(x)\n",
    "        x, hidden = self.gru(x, hidden)\n",
    "        x = self.out(x)\n",
    "        return x, hidden\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "   \n",
    "## Preparing Training Data\n",
    "        \n",
    "   + To train, for each pair we will need an input tensor (indexes of the words in the input sentence) and target tensor (indexes of the words in the target sentence). \n",
    "    \n",
    "   + While creating these vectors we will append the EOS token to both sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexFromSentence(lang, sentenc):\n",
    "    return [lang.wtoi[word] for word in sentenc.split(' ')]\n",
    "\n",
    "def tensorToSentence(lang, sentence):\n",
    "    ix = indexFromSentence(lang, sentence)\n",
    "    ix.append(E)\n",
    "\n",
    "    return torch.tensor(ix, dtype=torch.long, device=device).view(1, -1)\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    inputTensor = tensorsFromPair(inputLang, pair[0])\n",
    "    targtTensor = tensorsFromPair(outputLang, pair[1])\n",
    "\n",
    "    return (inputTensor, targtTensor)\n",
    "\n",
    "def getDataLoader(batchSiz):\n",
    "    inputLang, outputLang, pairs = prepareData('eng', 'fra', True)\n",
    "\n",
    "    n = len(pairs)\n",
    "    inputIds = np.zeros((n, MAX_LEN), dtype=np.int32)\n",
    "    tatrgtIds = np.zeros((n, MAX_LEN), dtype=np.int32)\n",
    "\n",
    "    for ix, (inp, targ) in enumerate(pairs):\n",
    "        inpIds = indexFromSentence(inputLang, inp)\n",
    "        targIds = indexFromSentence(outputLang, targ)\n",
    "        inpIds.append(E)\n",
    "        targIds.append(E)\n",
    "        inputIds[ix, :len(inpIds)] = inpIds\n",
    "        tatrgtIds[ix, :len(targIds)] = targIds \n",
    "    \n",
    "    trainData = TensorDataset(torch.LongTensor(inputIds).to(device), torch.LongTensor(tatrgtIds).to(device))\n",
    "\n",
    "    trainSample = RandomSampler(trainData)\n",
    "    trainDataLoader = DataLoader(trainData, sampler=trainSample, batch_size=batchSiz)\n",
    "\n",
    "    return inputLang, outputLang, trainDataLoader\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model\n",
    "   \n",
    "   + To train we run the input sentence through the encoder, and keep track of every output and the latest hidden state. \n",
    "   \n",
    "   + Then the decoder is given the <SOS> token as its first input, and the last hidden state of the encoder as its first hidden state.\n",
    "\n",
    "   + “Teacher forcing” is the concept of using the real target outputs as each next input, instead of using the decoder’s guess as the next input. \n",
    "   \n",
    "   + Using teacher forcing causes it to converge faster but when the trained network is exploited, it may exhibit instability.\n",
    "\n",
    "* You can observe outputs of teacher-forced networks that read with coherent grammar but wander far from the correct translation - intuitively it has learned to represent the output grammar and can “pick up” the meaning once the teacher tells it the first few words, but it has not properly learned how to create the sentence from the translation in the first place.\n",
    "\n",
    "* Because of the freedom PyTorch’s autograd gives us, we can randomly choose to use teacher forcing or not with a simple if statement. Turn teacher_forcing_ratio up to use more of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(dataloader, encoder, decoder, encoder_optimizer,\n",
    "          decoder_optimizer, criterion):\n",
    "\n",
    "    total_loss = 0\n",
    "    for data in dataloader:\n",
    "        input_tensor, target_tensor = data\n",
    "\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "        decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
    "\n",
    "        loss = criterion(\n",
    "            decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
    "            target_tensor.view(-1)\n",
    "        )\n",
    "        loss.backward()\n",
    "\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ This is a helper function to print time elapsed and estimated time remaining given the current time and progress %."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The whole training process looks like this:\n",
    "\n",
    "  + Start a timer\n",
    "\n",
    "  + Initialize optimizers and criterion\n",
    "\n",
    "  + Create set of training pairs\n",
    "\n",
    "  + Start empty losses array for plotting\n",
    "\n",
    "* Then we call train many times and occasionally print the progress (% of examples, time so far, estimated time) and average loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataloader, encoder, decoder, n_epochs, learning_rate=0.001,\n",
    "               print_every=100, plot_every=100):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if epoch % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, epoch / n_epochs),\n",
    "                                        epoch, epoch / n_epochs * 100, print_loss_avg))\n",
    "\n",
    "        if epoch % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting results\n",
    "\n",
    "  + Plotting is done with matplotlib, using the array of loss values plot_losses saved while training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "   \n",
    "   + Evaluation is mostly the same as training, but there are no targets so we simply feed the decoder’s predictions back to itself for each step. \n",
    "   \n",
    "   + Every time it predicts a word we add it to the output string, and if it predicts the EOS token we stop there. \n",
    "   \n",
    "   + We also store the decoder’s attention outputs for display later.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, input_lang, output_lang):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorToSentence(input_lang, sentence)\n",
    "\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "        decoder_outputs, decoder_hidden, decoder_attn = decoder(encoder_outputs, encoder_hidden)\n",
    "\n",
    "        _, topi = decoder_outputs.topk(1)\n",
    "        decoded_ids = topi.squeeze()\n",
    "\n",
    "        decoded_words = []\n",
    "        for idx in decoded_ids:\n",
    "            if idx.item() == E:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            decoded_words.append(output_lang.itow[idx.item()])\n",
    "    return decoded_words, decoder_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ We can evaluate random sentences from the training set and print out the input, target, and output to make some subjective quality judgements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, _ = evaluate(encoder, decoder, pair[0], inputLang, outputLang)\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Evaluating\n",
    "  \n",
    "  + With all these helper functions in place (it looks like extra work, but it makes it easier to run multiple experiments) we can actually initialize a network and start training.\n",
    "\n",
    "  + Remember that the input sentences were heavily filtered. \n",
    "  \n",
    "  + For this small dataset we can use relatively small networks of 256 hidden nodes and a single GRU layer. \n",
    "  \n",
    "  + After about 40 minutes on a MacBook CPU we’ll get some reasonable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 135842 sentence pairs\n",
      "Trimmed to 11445 sentence pairs after filtering\n",
      "Counting words...\n",
      "Counted words:\n",
      "fra 4601\n",
      "eng 2991\n",
      "1m 31s (- 1m 31s) (5 50%) 0.7135\n",
      "3m 3s (- 0m 0s) (10 100%) 0.5591\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 128\n",
    "batch_size = 32\n",
    "\n",
    "input_lang, output_lang, train_dataloader = getDataLoader(batch_size)\n",
    "\n",
    "encoder = EncoderRNN(input_lang.nwords, hidden_size).to(device)\n",
    "decoder = DecoderRNN(hidden_size, output_lang.nwords).to(device)\n",
    "\n",
    "train(train_dataloader, encoder, decoder, 10, print_every=5, plot_every=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### + Set dropout layers to eval mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> nous nous dirigeons dans la bonne direction\n",
      "= we re headed in the right direction\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "GRU: Expected input to be 2D or 3D, got 4D instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m encoder\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      2\u001b[0m decoder\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m----> 3\u001b[0m \u001b[43mevaluateRandomly\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[25], line 6\u001b[0m, in \u001b[0;36mevaluateRandomly\u001b[0;34m(encoder, decoder, n)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m>\u001b[39m\u001b[38;5;124m'\u001b[39m, pair[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m, pair[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m----> 6\u001b[0m output_words, _ \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpair\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputLang\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputLang\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m output_sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(output_words)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<\u001b[39m\u001b[38;5;124m'\u001b[39m, output_sentence)\n",
      "Cell \u001b[0;32mIn[23], line 6\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(encoder, decoder, sentence, input_lang, output_lang)\u001b[0m\n\u001b[1;32m      3\u001b[0m input_tensor \u001b[38;5;241m=\u001b[39m tensorToSentence(input_lang, sentence)\n\u001b[1;32m      5\u001b[0m encoder_outputs, encoder_hidden \u001b[38;5;241m=\u001b[39m encoder(input_tensor)\n\u001b[0;32m----> 6\u001b[0m decoder_outputs, decoder_hidden, decoder_attn \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_hidden\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m _, topi \u001b[38;5;241m=\u001b[39m decoder_outputs\u001b[38;5;241m.\u001b[39mtopk(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      9\u001b[0m decoded_ids \u001b[38;5;241m=\u001b[39m topi\u001b[38;5;241m.\u001b[39msqueeze()\n",
      "File \u001b[0;32m~/miniconda3/envs/tfenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tfenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[10], line 16\u001b[0m, in \u001b[0;36mDecoderRNN.forward\u001b[0;34m(self, encOutput, encHidden, targtTensor)\u001b[0m\n\u001b[1;32m     13\u001b[0m decodOutputs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(MAX_LEN):\n\u001b[0;32m---> 16\u001b[0m     decodOutput, decodHidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforwardStep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecodInput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecodHidden\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     decodOutputs\u001b[38;5;241m.\u001b[39mappend(decodOutput)\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m targtTensor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[10], line 34\u001b[0m, in \u001b[0;36mDecoderRNN.forwardStep\u001b[0;34m(self, x, hidden)\u001b[0m\n\u001b[1;32m     32\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39memb(x)\n\u001b[1;32m     33\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[0;32m---> 34\u001b[0m x, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgru\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout(x)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x, hidden\n",
      "File \u001b[0;32m~/miniconda3/envs/tfenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tfenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tfenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1104\u001b[0m, in \u001b[0;36mGRU.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m   1102\u001b[0m batch_sizes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m):\n\u001b[0;32m-> 1104\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGRU: Expected input to be 2D or 3D, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mD instead\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1105\u001b[0m is_batched \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m   1106\u001b[0m batch_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: GRU: Expected input to be 2D or 3D, got 4D instead"
     ]
    }
   ],
   "source": [
    "encoder.eval()\n",
    "decoder.eval()\n",
    "evaluateRandomly(encoder, decoder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
