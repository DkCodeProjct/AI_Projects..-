{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **NLP From Scratch**: Translation with a Sequence to Sequence Network and Attention\n",
    "  + In this project we will be teaching a neural network to translate from French to English.\n",
    "\n",
    "    -  [KEY: > input, = target, < output]\n",
    "      \n",
    "       ####  > il est en train de peindre un tableau .  \n",
    "       ####  = he is painting a picture .\n",
    "       ####  < he is painting a picture . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import  unicode_literals, print_function, division\n",
    "\n",
    "from io import open\n",
    "import unicodedata\n",
    "import re\n",
    "import random \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edit with Google Colab,, put images and explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = 0 #start of sentence\n",
    "E = 1 #end of sentnce\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, langName):\n",
    "        self.langName = langName\n",
    "        self.wtoi = {} #word to indx\n",
    "        self.wordToCount = {}\n",
    "        self.itow = {0:'<S>', 1:'<E'} # indx to word\n",
    "        self.nwords = 2\n",
    "    \n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "    \n",
    "    def addWord(self, word):\n",
    "        if word not in self.wtoi:\n",
    "            self.wtoi[word] = self.nwords\n",
    "            self.wordToCount[word] = 1\n",
    "            self.itow[self.nwords] = word\n",
    "            self.nwords += 1  # Count S and E\n",
    "        \n",
    "        else:\n",
    "            self.wordToCount[word] += 1\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The files are all in Unicode, to simplify we will turn Unicode characters to ASCII, make everything lowercase, and trim most punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# https://stackoverflow.com/a/518232/2809427\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "\n",
    "def normalizStr(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r'([.!?])', r\" \\1\", s)\n",
    "    s = re.sub(r'[^a-zA-Z!?]+', r' ', s)\n",
    "    return s.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To read the data file we will split the file into lines, and then split lines into pairs. The files are all English → Other Language, so if we want to translate from Other Language → English I added the reverse flag to reverse the pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def loadData(filePath, reverse=False):\n",
    "    pairs = []\n",
    "    with open(filePath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            eng, fre = line.strip().split('\\t') #english french\n",
    "\n",
    "            if reverse:\n",
    "                pairs.append([fre, eng])\n",
    "            \n",
    "            else:\n",
    "                pairs.append([eng, fre])\n",
    "\n",
    "    return pairs    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Since there are a lot of example sentences and we want to train something quickly, we’ll trim the data set to only relatively short and simple sentences. Here the maximum length is 10 words (that includes ending punctuation) and we’re filtering to sentences that translate to the form “I am” or “He is” etc. (accounting for apostrophes replaced earlier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 10\n",
    "\n",
    "engPrefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "def filtrPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LEN and len(p[1].split(' ')) < MAX_LEN and p[1].startswith(engPrefixes)\n",
    "\n",
    "def filtrPairs(pairs):\n",
    "    return [pair for pair in pairs if filtrPair(pair)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The full process for preparing the data is:\n",
    "   \n",
    "   + Read text file and split into lines, split lines into pairs\n",
    "\n",
    "   + Normalize text, filter by length and content\n",
    "\n",
    "   + Make word lists from sentences in pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizStr(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))  # Check if pairs were loaded correctly\n",
    "\n",
    "    pairs = filtrPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs after filtering\" % len(pairs))  # Check after filtering\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.langName, input_lang.nwords)\n",
    "    print(output_lang.langName, output_lang.nwords)\n",
    "    if not pairs:\n",
    "        print(\"Warning: No pairs found after filtering. Check data or filtering criteria.\")\n",
    "    \n",
    "    # The rest of your code for counting words etc.\n",
    "    return input_lang, output_lang, pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 135842 sentence pairs\n",
      "Trimmed to 11445 sentence pairs after filtering\n",
      "Counting words...\n",
      "Counted words:\n",
      "fra 4601\n",
      "eng 2991\n",
      "['je suis fidele', 'i m faithful']\n"
     ]
    }
   ],
   "source": [
    "inputLang, outputLang, pairs = prepareData('eng', 'fra', reverse=True)\n",
    "print(random.choice(pairs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, nInput, nHidden, dropoutP=0.1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.nHidden = nHidden\n",
    "\n",
    "        self.emb = nn.Embedding(nInput, nHidden)\n",
    "        self.gru = nn.GRU(nHidden, nHidden, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropoutP)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        emb = self.dropout(self.emb(x))\n",
    "        x, hidden = self.gru(emb)\n",
    "        return x, hidden\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, nHidden, nOutput):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.emb = nn.Embedding(nOutput, nHidden)\n",
    "        self.gru = nn.GRU(nHidden, nHidden, batch_first=True)\n",
    "        self.out = nn.Linear(nHidden, nOutput)\n",
    "    \n",
    "    def forward(self, encOutput, encHidden, targtTensor=None):\n",
    "        batchSiz = encOutput.size(0)\n",
    "        decodInput = torch.empty(batchSiz, 1, dtype=torch.long, device=device).fill_(S)\n",
    "        \n",
    "        decodHidden = encHidden\n",
    "        decodOutputs = []\n",
    "\n",
    "        for i in range(MAX_LEN):\n",
    "            decodOutput, decodHidden = self.forwardStep(decodInput, decodHidden)\n",
    "            decodOutputs.append(decodOutput)\n",
    "\n",
    "            if targtTensor is not None:\n",
    "                decodInput = targtTensor[:, i].unsqueeze(1)\n",
    "            \n",
    "            else:\n",
    "                _, topi = decodOutput.topk(i)\n",
    "                decodInput = topi.squeeze(-1).detach()\n",
    "        \n",
    "        decodOutputs = torch.cat(decodOutputs, dim=1)\n",
    "        decodOutputs = F.log_softmax(decodOutputs, dim=1)\n",
    "        \n",
    "        return decodOutputs, decodHidden, None \n",
    "\n",
    "    def forwardStep(self, x, hidden):\n",
    "        x = self.emb(x)\n",
    "        x = F.relu(x)\n",
    "        x, hidden = self.gru(x, hidden)\n",
    "        x = self.out(x)\n",
    "        return x, hidden\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(inputTensor, targtTensor, enc, decod, encOptim, decodOptim, criterion, maxLen=MAX_LEN, teacherForceRatio=0.5):\n",
    "    encOptim.zero_grad()\n",
    "    decodOptim.zero_grad()\n",
    "\n",
    "    loss = 0\n",
    "        \n",
    "    inputTensor = inputTensor.to(device)\n",
    "    targtTensor = targtTensor.to(device)\n",
    "\n",
    "    encHidden = enc.init_hidden(batch_size=inputTensor.size(0))\n",
    "    encOutput, encHidden = enc(inputTensor, encHidden)\n",
    "\n",
    "    decodHidden = encHidden\n",
    "    decodInput = torch.tensor([[S]] * inputTensor.size(0), device=device)\n",
    "\n",
    "    useTeacherForcing = True if torch.randn(1).item() < teacherForceRatio else False\n",
    "\n",
    "    if useTeacherForcing:\n",
    "        for i in range(targtTensor.size(1)):\n",
    "            decodOutput, decodHidden = decod(decodInput, decodHidden)\n",
    "            loss += criterion(decodOutput, targtTensor[:, i])\n",
    "            decodInput = targtTensor[:, i].unsqueeze(1)\n",
    "    \n",
    "    else:\n",
    "        for i in range(targtTensor.size(1)):\n",
    "            decodOutput, decodHidden = decod(decodInput, decodHidden)\n",
    "            loss += criterion(decodOutput, targtTensor[:, 1])\n",
    "\n",
    "            _, topi = decodInput.topk(1)\n",
    "            decodOutput = topi.squeeze(-1).detach()\n",
    "    \n",
    "    loss.backward()\n",
    "\n",
    "    encOptim.step()\n",
    "    decodOptim.step()\n",
    "\n",
    "    return loss.item() / targtTensor.size(1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(maxSteps):\n\u001b[1;32m     18\u001b[0m     inputTensor, targtTensor \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mchoice(pairs)\n\u001b[0;32m---> 19\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputTensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargtTensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencOptim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecodOptim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmaxSteps\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[48], line 8\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(inputTensor, targtTensor, enc, decod, encOptim, decodOptim, criterion, maxLen, teacherForceRatio)\u001b[0m\n\u001b[1;32m      3\u001b[0m decodOptim\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m      5\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 8\u001b[0m inputTensor \u001b[38;5;241m=\u001b[39m \u001b[43minputTensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m(device)\n\u001b[1;32m      9\u001b[0m targtTensor \u001b[38;5;241m=\u001b[39m targtTensor\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     11\u001b[0m encHidden \u001b[38;5;241m=\u001b[39m enc\u001b[38;5;241m.\u001b[39minit_hidden(batch_size\u001b[38;5;241m=\u001b[39minputTensor\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "inputSiz = inputLang.nwords\n",
    "outputSiz = outputLang.nwords\n",
    "\n",
    "hiddenSiz = 256\n",
    "teacherForcingRate = 0.5\n",
    "maxSteps = 1000\n",
    "lr = 0.01\n",
    "\n",
    "# Initialize models, optimizers, and loss function\n",
    "enc = EncoderRNN(inputSiz, hiddenSiz).to(device)\n",
    "decod = DecoderRNN(hiddenSiz, outputSiz).to(device)\n",
    "\n",
    "encOptim = optim.SGD(enc.parameters(), lr=lr)\n",
    "decodOptim = optim.SGD(decod.parameters(), lr=lr)\n",
    "criterion = nn.NLLLoss() # Negative Log Likelihood Loss\n",
    "\n",
    "for i in range(maxSteps):\n",
    "    inputTensor, targtTensor = random.choice(pairs)\n",
    "    loss = train(inputTensor, targtTensor, enc, decod, encOptim, decodOptim, criterion)\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(f\"Epoch {i}/{maxSteps}, Loss: {loss:.4f}\")\n",
    "    break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
