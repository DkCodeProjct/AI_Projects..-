{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's build GPT: from scratch, in code, spelled out.\n",
    "\n",
    "### :/ Andrej Karpathy Lecture::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import numpy\n",
    "import re\n",
    "import unicodedata\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as file:\n",
    "    txt = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of dataset in chars 1115394\n"
     ]
    }
   ],
   "source": [
    "print('Len of dataset in chars', len(txt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you\n",
      "................\n"
     ]
    }
   ],
   "source": [
    "print(txt[:200])\n",
    "print('................')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(txt)))\n",
    "vocabSiz = len(chars)\n",
    "\n",
    "print(''.join(chars))\n",
    "print(vocabSiz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### encoder: take a str, output a list of int\n",
    "\n",
    "### decoder: take a list of int, output a str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = {ch:i for i, ch in enumerate(chars)}\n",
    "itos = {i:ch for i, ch in enumerate(chars)}\n",
    "enc = lambda s: [stoi[c] for c in s]\n",
    "decod = lambda l: ''.join([itos[i] for i in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hii thre\n"
     ]
    }
   ],
   "source": [
    "print(enc('hii there'))\n",
    "print(decod(enc('hii thre')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### enc entire dataset and store in torch.Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) <built-in method type of Tensor object at 0x7520a030fba0>\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(enc(txt), dtype=torch.long)\n",
    "print(data.shape, data.type)\n",
    "\n",
    "print(data[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data\n",
    " \n",
    " + 90% train, others dev/val "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9*len(data))\n",
    "trainData = data[:n]\n",
    "devData = data[n:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BlockSize/ ContextLength: max len of the char/token that could feed to trannsformer\n",
    "blockSiz = 8\n",
    "trainData[:blockSiz+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So the reason why [ y ] have skip sort of: [1:+1]\n",
    "  \n",
    "   + is, lets say ''x'' is, ['what is your name']\n",
    "\n",
    "   + then ''y'' would be ['is your name']:\n",
    "\n",
    "   *  cos is come after what, the model predicting the next token\n",
    "\n",
    "++\n",
    "\n",
    "#### So now if x [18, 47], y would predict [56], Cos its the next token and so forth "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]), output is 47 \n",
      "when input is tensor([18, 47]), output is 56 \n",
      "when input is tensor([18, 47, 56]), output is 57 \n",
      "when input is tensor([18, 47, 56, 57]), output is 58 \n",
      "when input is tensor([18, 47, 56, 57, 58]), output is 1 \n",
      "when input is tensor([18, 47, 56, 57, 58,  1]), output is 15 \n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]), output is 47 \n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]), output is 58 \n"
     ]
    }
   ],
   "source": [
    "x = trainData[:blockSiz]\n",
    "y = trainData[1:blockSiz+1]\n",
    "\n",
    "for token in range(blockSiz):\n",
    "    contxt = x[:token+1]\n",
    "    targt = y[token]\n",
    "\n",
    "    print(f'when input is {contxt}, output is {targt} ')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Dim\n",
    "\n",
    " + Batch Size: \n",
    "    * how many indepndnt seq will be porcess in parallel\n",
    " \n",
    " + Block Size:\n",
    "    * what is the max contxt len for prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "targts\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "\n",
      ".....\n",
      "when input is tensor([24]), output is 43\n",
      "when input is tensor([24, 43]), output is 58\n",
      "when input is tensor([24, 43, 58]), output is 5\n",
      "when input is tensor([24, 43, 58,  5]), output is 57\n",
      "when input is tensor([24, 43, 58,  5, 57]), output is 1\n",
      "when input is tensor([24, 43, 58,  5, 57,  1]), output is 46\n",
      "when input is tensor([24, 43, 58,  5, 57,  1, 46]), output is 43\n",
      "when input is tensor([24, 43, 58,  5, 57,  1, 46, 43]), output is 39\n",
      "when input is tensor([44]), output is 53\n",
      "when input is tensor([44, 53]), output is 56\n",
      "when input is tensor([44, 53, 56]), output is 1\n",
      "when input is tensor([44, 53, 56,  1]), output is 58\n",
      "when input is tensor([44, 53, 56,  1, 58]), output is 46\n",
      "when input is tensor([44, 53, 56,  1, 58, 46]), output is 39\n",
      "when input is tensor([44, 53, 56,  1, 58, 46, 39]), output is 58\n",
      "when input is tensor([44, 53, 56,  1, 58, 46, 39, 58]), output is 1\n",
      "when input is tensor([52]), output is 58\n",
      "when input is tensor([52, 58]), output is 1\n",
      "when input is tensor([52, 58,  1]), output is 58\n",
      "when input is tensor([52, 58,  1, 58]), output is 46\n",
      "when input is tensor([52, 58,  1, 58, 46]), output is 39\n",
      "when input is tensor([52, 58,  1, 58, 46, 39]), output is 58\n",
      "when input is tensor([52, 58,  1, 58, 46, 39, 58]), output is 1\n",
      "when input is tensor([52, 58,  1, 58, 46, 39, 58,  1]), output is 46\n",
      "when input is tensor([25]), output is 17\n",
      "when input is tensor([25, 17]), output is 27\n",
      "when input is tensor([25, 17, 27]), output is 10\n",
      "when input is tensor([25, 17, 27, 10]), output is 0\n",
      "when input is tensor([25, 17, 27, 10,  0]), output is 21\n",
      "when input is tensor([25, 17, 27, 10,  0, 21]), output is 1\n",
      "when input is tensor([25, 17, 27, 10,  0, 21,  1]), output is 54\n",
      "when input is tensor([25, 17, 27, 10,  0, 21,  1, 54]), output is 39\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "bathSiz = 4\n",
    "blockSiz = 8\n",
    "\n",
    "def getBatch(split):\n",
    "    data = trainData if split == 'train' else devData\n",
    "    ix = torch.randint(len(data) - blockSiz, (bathSiz, ))\n",
    "    x = torch.stack([data[i:i+blockSiz] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+blockSiz+1] for i in ix])\n",
    "\n",
    "    return x, y\n",
    "\n",
    "xb, yb = getBatch('train')\n",
    "print('inputs')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "\n",
    "print('targts')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('\\n.....')\n",
    "\n",
    "for batch in range(bathSiz):\n",
    "    for tokn in range(blockSiz):\n",
    "        contxt = xb[batch, :tokn+1]\n",
    "        targt = yb[batch, tokn]\n",
    "        print(f'when input is {contxt}, output is {targt}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n"
     ]
    }
   ],
   "source": [
    "# our input to transF\n",
    "print(xb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigram Language Model,\n",
    "\n",
    " \n",
    "   * ## Embedding:\n",
    "     + so, emb layer will \"plucks out\" rows corresponding to token indices in your input.\n",
    "\n",
    "     + Ex: ['whats your name'], emb will get conrespoding token for this input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B T C, arragment:\n",
    "\n",
    "  * Why This Arrangement?\n",
    "     \n",
    "     + The (B, T, C) shape allows models to handle mini-batches of sequences, where:\n",
    "     \n",
    "     + Each batch contains multiple sequences.\n",
    "     \n",
    "     + Each sequence has multiple tokens.\n",
    "     \n",
    "     + Each token is represented by an embedding vector.\n",
    "\n",
    "  \n",
    "### * Example Walkthrough\n",
    "   \n",
    "   + Suppose:\n",
    "\n",
    "      - **Vocabulary size (vocab_size)**: 10 \n",
    "      \n",
    "      - **Embedding size (C)**: 4 (each token is mapped to a 4-dimensional vector)\n",
    "      \n",
    "      - **Batch size (B)**: 2 (you process 2 sequences at once)\n",
    "      \n",
    "      - **Sequence length (T)**: 3 (each sequence has 3 tokens)  \n",
    "\n",
    "\n",
    "###### ----------\n",
    "         ix = [\n",
    "            [1, 3, 5],  # First sequence in the batch\n",
    "            [0, 4, 2]   # Second sequence in the batch\n",
    "         ]\n",
    "###### ----------\n",
    "\n",
    "  * This input tensor has the shape (2, 3):\n",
    "      \n",
    "      + B = 2: Two sequences (first and second).\n",
    "      \n",
    "      + T = 3: Three tokens in each sequence.\n",
    "\n",
    "  \n",
    "  *  Embedding Lookup   \n",
    "   - The embedding layer \"plucks out\" embeddings for each token index:\n",
    "\n",
    "         - For token 1 → [0.1, 0.2, 0.3, 0.4]\n",
    "         - For token 3 → [0.5, 0.6, 0.7, 0.8]\n",
    "         - For token 5 → [0.9, 1.0, 1.1, 1.2], etc.\n",
    "\n",
    "### * The output is a (B, T, C) tensor:\n",
    "\n",
    " \n",
    "            output = [\n",
    "            [  # First sequence embeddings\n",
    "               [0.1, 0.2, 0.3, 0.4],  # Embedding for token 1\n",
    "               [0.5, 0.6, 0.7, 0.8],  # Embedding for token 3\n",
    "               [0.9, 1.0, 1.1, 1.2]   # Embedding for token 5\n",
    "            ],\n",
    "            [  # Second sequence embeddings\n",
    "               [0.01, 0.02, 0.03, 0.04],  # Embedding for token 0\n",
    "               [0.5, 0.6, 0.7, 0.8],      # Embedding for token 4\n",
    "               [0.9, 1.0, 1.1, 1.2]       # Embedding for token 2\n",
    "            ]\n",
    "            ]\n",
    "\n",
    "\n",
    "### Shape: (2, 3, 4) where:\n",
    "\n",
    "   + B=2 → 2 sequences.\n",
    "   \n",
    "   + T=3 → 3 tokens per sequence.\n",
    "      \n",
    "   + C=4 → Each token has a 4-dimensional embedding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So in our case its [4, 8, 65]\n",
    "  \n",
    "  + where [ **2** ] is the seqLen or **Batch Size**\n",
    "\n",
    "  + and [ **8** ] repr **Block Size**\n",
    "\n",
    "  + lastly [ **65**]   is for, how many chars, vocab Size, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B T C ERROR:\n",
    "  + however when we do this:\n",
    "\n",
    "\n",
    "            return logits, loss\n",
    "            logits, loss = model(xb, yb)  \n",
    "\n",
    "\n",
    "  + we get error Cos,\n",
    "\n",
    "\n",
    "      + RuntimeError: Expected target size [4, 65], got [4, 8]\n",
    "\n",
    "\n",
    "  + Cos Torch expects B C T, not B T C like we have now , \n",
    "  its just the detail how torch treat these kind of inputs,\n",
    "  so we not gonna deal with that \n",
    "\n",
    "\n",
    "  + instead what we gonna is **Reshape out Logits**\n",
    "  \n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocabSiz):\n",
    "        super(BigramLanguageModel, self).__init__()\n",
    "        self.tokenEmbTable = nn.Embedding(vocabSiz, vocabSiz) # 65x65\n",
    "    \n",
    "    def forward(self, ix, targt=None):\n",
    "        #indx and target are both (B, T) tensor or int\n",
    "        logits = self.tokenEmbTable(ix) # (B, T, C)\n",
    "        \n",
    "        if targt is None:\n",
    "            loss = None\n",
    "        \n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targt = targt.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targt)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def genarate(self, ix, maxNewTokn):\n",
    "        #ix is (B,T) array of indeceis in currnt contxt\n",
    "        \n",
    "        for i in range(maxNewTokn):\n",
    "            logits, loss = self(ix)\n",
    "\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] #become (B, C)\n",
    "\n",
    "            probs = F.softmax(logits, dim=1) # (B, C)\n",
    "\n",
    "            # sample From the Distribution\n",
    "            ixNext = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "\n",
    "            # append sample indx to the running seq \n",
    "            ix = torch.cat((ix, ixNext), dim=1) # (B, T+1)\n",
    "        \n",
    "        return ix\n",
    "    \n",
    "model = BigramLanguageModel(vocabSiz)\n",
    "logits, loss = model(xb, yb)\n",
    "\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _________________________________________________________\n",
    "\n",
    "## Also we can guess what the NLLL Loss Should Be, \n",
    "\n",
    "  + Cos, \n",
    "\n",
    "         -ln(1/65) \n",
    "  \n",
    "  \n",
    "  + log or ln, l / 65 and negative of it is [ **4.17**]\n",
    "\n",
    "  + But what we got is  [ **4.87**]\n",
    "\n",
    "\n",
    "### * whats this mean is\n",
    "  \n",
    "  + The initial Predictions are not **Super Diffuse**, \n",
    "  \n",
    "  + they've got Little Bit **Entrophy**\n",
    "  and so we guessing Wrong\n",
    "\n",
    "\n",
    "##### Diffuse Mean:\n",
    "  + **how well** the model's **predictions are spread out or distributed** across the possible outcomes\n",
    "\n",
    "# _________________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"
     ]
    }
   ],
   "source": [
    "print(decod(model.genarate(ix=torch.zeros((1, 1), dtype=torch.long), maxNewTokn=100)[0].tolist()))\n",
    "# abviously These Sampling are Garbage, so we would Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train...//"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dkcode/miniconda3/envs/tfenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Create Optimizer\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.453843355178833\n"
     ]
    }
   ],
   "source": [
    "batchSiz = 32\n",
    "\n",
    "for i in range(10000):\n",
    "    xb, yb = getBatch('train')\n",
    "\n",
    "    logits, loss = model(xb, yb)\n",
    "    optim.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OTENRBESI RINous O:\n",
      "Bupouly! ff lathe tat ter jur\n",
      "ELYoraty,\n",
      "I sper tornd be ho t helin, t masu kivin\n"
     ]
    }
   ],
   "source": [
    "print(decod(model.genarate(ix=torch.zeros((1, 1), dtype=torch.long), maxNewTokn=100)[0].tolist()))\n",
    "# Whoo, now we get some Less grabage shit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
