{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PRSgek_8iTNH",
        "outputId": "dab4b800-2f7d-4c21-e5bc-2643f2da6932"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1084077\n",
            "113\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['\\n',\n",
              " ' ',\n",
              " '!',\n",
              " '&',\n",
              " '(',\n",
              " ')',\n",
              " ',',\n",
              " '-',\n",
              " '.',\n",
              " '0',\n",
              " '1',\n",
              " '2',\n",
              " '3',\n",
              " '4',\n",
              " '5',\n",
              " '6',\n",
              " '7',\n",
              " '8',\n",
              " '9',\n",
              " ':',\n",
              " ';',\n",
              " '=',\n",
              " '?',\n",
              " 'A',\n",
              " 'B',\n",
              " 'C',\n",
              " 'D',\n",
              " 'E',\n",
              " 'F',\n",
              " 'G',\n",
              " 'H',\n",
              " 'I',\n",
              " 'J',\n",
              " 'K',\n",
              " 'L',\n",
              " 'M',\n",
              " 'N',\n",
              " 'O',\n",
              " 'P',\n",
              " 'Q',\n",
              " 'R',\n",
              " 'S',\n",
              " 'T',\n",
              " 'U',\n",
              " 'V',\n",
              " 'W',\n",
              " 'X',\n",
              " 'Y',\n",
              " 'Z',\n",
              " '[',\n",
              " ']',\n",
              " '_',\n",
              " 'a',\n",
              " 'b',\n",
              " 'c',\n",
              " 'd',\n",
              " 'e',\n",
              " 'f',\n",
              " 'g',\n",
              " 'h',\n",
              " 'i',\n",
              " 'j',\n",
              " 'k',\n",
              " 'l',\n",
              " 'm',\n",
              " 'n',\n",
              " 'o',\n",
              " 'p',\n",
              " 'q',\n",
              " 'r',\n",
              " 's',\n",
              " 't',\n",
              " 'u',\n",
              " 'v',\n",
              " 'w',\n",
              " 'x',\n",
              " 'y',\n",
              " 'z',\n",
              " '·',\n",
              " 'À',\n",
              " 'Â',\n",
              " 'Æ',\n",
              " 'É',\n",
              " 'Ü',\n",
              " 'à',\n",
              " 'ä',\n",
              " 'æ',\n",
              " 'ç',\n",
              " 'è',\n",
              " 'é',\n",
              " 'ê',\n",
              " 'î',\n",
              " 'ï',\n",
              " 'ó',\n",
              " 'ô',\n",
              " 'û',\n",
              " 'ü',\n",
              " 'Œ',\n",
              " 'œ',\n",
              " '̓',\n",
              " 'Ψ',\n",
              " 'έ',\n",
              " 'ν',\n",
              " 'ς',\n",
              " 'υ',\n",
              " 'χ',\n",
              " '–',\n",
              " '—',\n",
              " '‘',\n",
              " '’',\n",
              " '“',\n",
              " '”',\n",
              " '\\ufeff']"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "# sigmund frued interpretation of Dreames book\n",
        "with open('/content/pg66048.txt', 'r', encoding='utf-8') as file:\n",
        "    txt = file.read()\n",
        "print(len(txt))\n",
        "\n",
        "chars = sorted(list(set(txt)))\n",
        "print(len(chars))\n",
        "chars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "h9exPJ4niTNM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ab49b73-aa5a-4129-cb89-d807ae96c9f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step/ 0: train loss 4.8604, dev/val loss 4.8554\n",
            "step/ 500: train loss 2.4237, dev/val loss 2.4054\n",
            "step/ 1000: train loss 1.9636, dev/val loss 1.9318\n",
            "step/ 1500: train loss 1.8961, dev/val loss 1.8572\n",
            "step/ 2000: train loss 1.8986, dev/val loss 1.8647\n",
            "step/ 2500: train loss 2.1981, dev/val loss 2.1678\n",
            "step/ 3000: train loss 2.3962, dev/val loss 2.3645\n",
            "step/ 3500: train loss 2.5077, dev/val loss 2.4770\n",
            "step/ 3999: train loss 2.5634, dev/val loss 2.5387\n",
            "Final model Save at  [save_gpt_model.pt]\n",
            "\n",
            "igtotonuiantrll d oar sson\n",
            "t n p ot h ton f thestes aie ndmait  oigexctand se miche scanhe; tif rel t dit to arecim dl ‘xamp atosseht ion hodecols rtioiakehe tnronde\n",
            "m\n",
            "os. surasaren an, o ytiat cofel almorofses\n",
            "rex at nfes—ds trutor wc]ss onapsyofinty, r wshyrul ice t che _Nymar amoly whr r cat sot tens alol pu bis s inulimaisere Bulincus\n",
            "stecnge aveaie “ret”ldrd)erthouchofrestit vilfte fiintiofou. rhon cationt owe berengys hinthan. od tho s i iundurye\n",
            "yreshee\n",
            "otangur, Mocit ofiv,\n",
            "ppa\n",
            "hing thisa\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10001"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# ------------\n",
        "# // hyperPara\n",
        "bathSiz = 54\n",
        "blockSiz = 220\n",
        "epochs = 4000\n",
        "evalIntervals = 500\n",
        "lr = 3e-3\n",
        "device = \"cuda\"  if torch.cuda.is_available() else \"cpu\"\n",
        "evalItrs = 200\n",
        "nEmb = 354\n",
        "nHead = 5\n",
        "nLayers = 5\n",
        "dropout = 0.1\n",
        "# -----------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# sigmund frued Dreames book\n",
        "with open('/content/pg66048.txt', 'r', encoding='utf-8') as file:\n",
        "    txt = file.read()\n",
        "\n",
        "chars = sorted(list(set(txt)))\n",
        "vocabSiz = len(chars)\n",
        "\n",
        "# mapping chars\n",
        "stoi = {ch:i for i, ch in enumerate(chars)}\n",
        "itos = {i:ch for i, ch in enumerate(chars)}\n",
        "enc = lambda s: [stoi[c] for c in s]\n",
        "decod = lambda l: \"\".join([itos[i] for i in l])\n",
        "\n",
        "# train_test_split\n",
        "data = torch.tensor(enc(txt), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% train, rest dev/val\n",
        "trainData = data[:n]\n",
        "devData  = data[n:]\n",
        "\n",
        "# loading data\n",
        "def getBatch(split):\n",
        "    data = trainData if split == 'train' else devData\n",
        "    ix = torch.randint(len(data) - blockSiz, (bathSiz, )) # get random data chunks\n",
        "    x = torch.stack([data[i:i+blockSiz] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+blockSiz+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "\n",
        "    return x, y\n",
        "\n",
        "\n",
        "# loss estimate\n",
        "@torch.no_grad()\n",
        "def estimateLoss():\n",
        "    out = { }\n",
        "    # put model on evaluation , train mode\n",
        "    model.eval()\n",
        "    for split in [\"train\", \"dev\"]:\n",
        "        losses = torch.zeros(evalItrs)\n",
        "\n",
        "        for k in range(evalItrs):\n",
        "            X, Y = getBatch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "\n",
        "        out[split] = losses.mean()\n",
        "\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "    def __init__(self, headSiz):\n",
        "        super(Head, self).__init__()\n",
        "        self.key = nn.Linear(nEmb, headSiz, bias=False)\n",
        "        self.quary = nn.Linear(nEmb, headSiz, bias=False)\n",
        "        self.value = nn.Linear(nEmb, headSiz, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(blockSiz, blockSiz)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)\n",
        "        q = self.quary(x)\n",
        "\n",
        "        # compute attention scores (\"affinities\")\n",
        "        # read the papaer to see the eqation\n",
        "        w = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5 #(B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
        "\n",
        "        # fill all tensor val with -inf where its == 0\n",
        "        w = w.masked_fill(self.tril[:T, :T] == 0, float('-inf')) #(B, T, T)\n",
        "        w = F.softmax(w, dim=-1) # (B, T, T)\n",
        "        w = self.dropout(w)\n",
        "\n",
        "        v = self.value(x)\n",
        "        out = w @ v\n",
        "\n",
        "        return out\n",
        "\n",
        "class MultiHeadAteention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "    def __init__(self, nHead, headSiz):\n",
        "        super(MultiHeadAteention, self).__init__()\n",
        "        self.heads = nn.ModuleList([Head(headSiz) for _ in range(nHead)])\n",
        "\n",
        "        # projection: combining the outputs of all attention heads into a unified representation\n",
        "        self.projection = nn.Linear(headSiz * nHead, nEmb)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)#dim=C chanal\n",
        "        out = self.dropout(self.projection(out))\n",
        "        return out\n",
        "\n",
        "class FeedForwardNetwork(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "    def __init__(self, nEmb):\n",
        "        super(FeedForwardNetwork, self).__init__()\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            # in paper they mul by 4\n",
        "            nn.Linear(nEmb, 4 * nEmb),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * nEmb, nEmb),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "# look at the diagram on paper\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, nEmb, nHead):\n",
        "        super(Block, self).__init__()\n",
        "        headSiz =nEmb // nHead\n",
        "        self.selfAtn = MultiHeadAteention(nHead, headSiz)\n",
        "        self.ffn = FeedForwardNetwork(nEmb)\n",
        "        self.layrNorm_1 = nn.LayerNorm(nEmb)\n",
        "        self.layrNorm_2 = nn.LayerNorm(nEmb)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x + == risidual Connections\n",
        "        x = x + self.selfAtn(self.layrNorm_1(x))\n",
        "        x = x + self.ffn(self.layrNorm_2(x))\n",
        "        return x\n",
        "\n",
        "class GPTLanguageModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GPTLanguageModel, self).__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.toknEmbTable = nn.Embedding(vocabSiz, nEmb)\n",
        "        self.posEmbTable = nn.Embedding(blockSiz, nEmb)\n",
        "        self.blocks = nn.Sequential(*[Block(nEmb, nHead) for i in range(nLayers)])\n",
        "        self.lyrNormFinl = nn.LayerNorm(nEmb)\n",
        "        self.lmHead = nn.Linear(nEmb, vocabSiz)\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    #  init the W and biases of specific layers (nn.Linear and nn.Embedding)\n",
        "    def _init_weights(self, mdule):\n",
        "        if isinstance(mdule, nn.Linear):\n",
        "            torch.nn.init.normal_(mdule.weight, mean=0.0, std=0.02)\n",
        "\n",
        "            if mdule.bias is not None:\n",
        "                torch.nn.init.zeros_(mdule.bias)\n",
        "\n",
        "        elif isinstance(mdule, nn.Embedding):\n",
        "            torch.nn.init.normal_(mdule.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, ix, targt=None):\n",
        "        B, T = ix.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        toknEmb = self.toknEmbTable(ix) # (B, T, C)\n",
        "        posEmb = self.posEmbTable(torch.arange(T, device=device)) # (T, C)\n",
        "        x = toknEmb + posEmb # (B, T, C)\n",
        "        x = self.blocks(x) # (B, T, C)\n",
        "        x = self.lyrNormFinl(x) #(B, T, C)\n",
        "        logits = self.lmHead(x) #(B, T, vocabSiz)\n",
        "\n",
        "        if targt is None:\n",
        "            loss = None\n",
        "\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targt = targt.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targt)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def genarate(self, ix, maxNewTokn):\n",
        "        for _ in range(maxNewTokn):\n",
        "            # crop idx to the last block_size tokens\n",
        "            ixCond = ix[:, -blockSiz:]\n",
        "\n",
        "            #get predict\n",
        "            logits, loss =self(ixCond)\n",
        "\n",
        "            # focus only last time step\n",
        "            logits = logits[:, -1, :] #become (B, C)\n",
        "\n",
        "            probs = F.softmax(logits, dim=-1) #(B, C)\n",
        "\n",
        "            ixNxt = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "\n",
        "            ix = torch.cat((ix, ixNxt), dim=1) #(B, T+1)\n",
        "        return ix\n",
        "\n",
        "model = GPTLanguageModel()\n",
        "m = model.to(device)\n",
        "\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "\n",
        "for i in range(epochs):\n",
        "    if i % evalIntervals == 0 or i == epochs - 1:\n",
        "        losses = estimateLoss()\n",
        "        print(f\"step/ {i}: train loss {losses['train']:.4f}, dev/val loss {losses['dev']:.4f}\")\n",
        "\n",
        "    # Save the model at evaluation intervals\n",
        "    #checkPointPath = f\"modelCheckPoinEpoch_{i}.pt\"\n",
        "    #torch.save(model.state_dict(), checkPointPath)\n",
        "    #print(f\"final model at {checkPointPath}\")\n",
        "\n",
        "    xb, yb = getBatch('train')\n",
        "    logits, loss = model(xb, yb)\n",
        "    optim.zero_grad()\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "\n",
        "torch.save(model.state_dict(), 'save_gpt_model.pt')\n",
        "print(\"Final model Save at  [save_gpt_model.pt]\")\n",
        "\n",
        "# generate/sample from the model\n",
        "contxt = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decod(m.genarate(contxt, maxNewTokn=500)[0].tolist()))\n",
        "open('gptModelGen.txt', 'w').write(decod(m.genarate(contxt, maxNewTokn=10000)[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Total Train Time >:\n",
        "  * 25m on a T4 GPU Google Colab.\n",
        "  * Loss train, val = 2.5\n",
        "\n",
        "## Ok so ive Mistake\n",
        "  * so basicaly ive look at Andrej Code and writing base on it\n",
        "  * However ive forgot to add ix[:, -blocksiz:]\n",
        "  * so ive to retrain the model for 20 min agin,\n",
        "\n",
        "# What to take on this project:\n",
        "  * Make sure to run 2 or 3 epochs, genarate samples.\n",
        "\n",
        "  * then Get some confident that this model has no bug..\n",
        "\n",
        "  * Dont just train for 20 min and Face the Error in Post train,,\n",
        "\n",
        "\n",
        "### Also the Model genarations are trash as well,\n",
        "\n",
        "# So, You have to do 1000 Mistakes to Be good...!!!\n",
        "  * Next time when i train id have Some Confident\n",
        "  * Also i worried about the GPU Use Limitation on Colab\n"
      ],
      "metadata": {
        "id": "4yW1YFGCsV1u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(sum(p.numel() for p in model.parameters()))\n"
      ],
      "metadata": {
        "id": "oUJnsTI3j9nV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae40336d-9cbc-4f15-f277-fbb6b2d41468"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7667045\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generate/sample from the model\n",
        "contxt = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decod(m.genarate(contxt, maxNewTokn=1000)[0].tolist()))\n",
        "open('gptModelGen.txt', 'w').write(decod(m.genarate(contxt, maxNewTokn=10000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G520BNun0wBk",
        "outputId": "26321ee1-63c3-4c40-b979-bd50e707e516"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "trathinsd eams thagerirg\n",
            "t, ofic l o agly fr.llosear h at d o tstheris sioated ngroe ses thearnouserertin e\n",
            "adiers tthe avis te m ay ongo tifamo\n",
            " d nge. cen herr heFt reand fas otowefish am, tik  tamsitha sechas ttanf ticonouavemase ondreware ily s whichie. atelldry conathansthinof fois du ram th ale wo thanes “t renhe pego\n",
            "pe shand. senhe.y abereay tove ass s igind s the atr is Itose con a\n",
            "ixplll\n",
            "s otse ppeleas I dertes\n",
            "psionbas cctwoures, f mentoo istof ps. thone, th citisichalls Hexasd? o-th dslwhiche d ps feretgexsift, oforichexanicideshamsihedy _Tasse omivowhonediecllicheat\n",
            "d non al\n",
            "aseres. ingold. (montedinom supry. ora I tico ouce larallna co[Yan ndll T, the pe acofasf ufintin ted tolelfalanawthecathears hicio och sh h p.\n",
            "Ial im alltee tuowhan he thead me fie\n",
            "mpifavere thawofoticith therind Hlof ritofod ocagl Ahe sem. opa incche adon idrad ts o beaur\n",
            "teralgsie\n",
            "bell chay, tharinis th pethien fe bud p oupre\n",
            "musp hed tsutrm;\n",
            "tho d m cho als wath gf luathe I cedich inererr Tptes\n",
            "fee\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10001"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "84NvUtsl0yD7"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}