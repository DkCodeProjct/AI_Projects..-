{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n"
      ],
      "metadata": {
        "id": "vsiM-IITbATf"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ----------\n",
        "# Hyper Para\n",
        "#blockSiz = 256\n",
        "#batchSiz = 64\n",
        "#epochs = 5000\n",
        "#evalInterval = 500\n",
        "#lr = 3e-4\n",
        "#device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "#evalIters = 200\n",
        "#nEmb = 384\n",
        "#nHead = 6\n",
        "#nLayers = 5\n",
        "#dropout = 0.2\n",
        "# ----------\n",
        "\n",
        "\n",
        "# Descaled Hyper para\n",
        "blockSiz = 84\n",
        "batchSiz = 30\n",
        "epochs = 4000\n",
        "evalInterval = 500\n",
        "nEmb = 160\n",
        "nHead = 3\n",
        "nLayers = 3\n",
        "lr = 3e-2\n",
        "dropout = 0.1\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "evalIters = 200\n",
        "#-------------\n"
      ],
      "metadata": {
        "id": "EvR9lPi0bAo8"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "with open('pg66048.txt', 'r', encoding='utf-8') as file:\n",
        "    txt = file.read()\n",
        "\n",
        "\n",
        "chars = sorted(list(set(txt)))\n",
        "vocabSiz = len(chars)\n",
        "\n",
        "stoi = {ch:i for i, ch in enumerate(chars)}\n",
        "itos = {i:ch for i, ch in enumerate(chars)}\n",
        "enc = lambda s: [stoi[c] for c in s ]\n",
        "decod = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "data = torch.tensor(enc(txt), dtype=torch.long)\n",
        "n = int(0.9*len(data))\n",
        "\n",
        "trainData = data[:n]\n",
        "devData = data[n:]\n",
        "\n",
        "def getBatch(split):\n",
        "    data = trainData if split == 'train' else devData\n",
        "    ix = torch.randint(len(data) - blockSiz, (batchSiz, ))\n",
        "    x = torch.stack([data[i:i+blockSiz] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+blockSiz+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "QKtF65GWbFvM"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(chars)\n",
        "print(len(chars))"
      ],
      "metadata": {
        "id": "T2DpKVoked6a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71a26c04-a240-41dd-c2f3-04555d79147e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\n', ' ', '!', '&', '(', ')', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '·', 'À', 'Â', 'Æ', 'É', 'Ü', 'à', 'ä', 'æ', 'ç', 'è', 'é', 'ê', 'î', 'ï', 'ó', 'ô', 'û', 'ü', 'Œ', 'œ', '̓', 'Ψ', 'έ', 'ν', 'ς', 'υ', 'χ', '–', '—', '‘', '’', '“', '”', '\\ufeff']\n",
            "113\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "@torch.no_grad()\n",
        "def estimateLoss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'dev']:\n",
        "        losses = torch.zeros(evalIters)\n",
        "        for k in range(evalIters):\n",
        "            X, Y = getBatch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    def __init__(self, headSiz):\n",
        "        super(Head, self).__init__()\n",
        "        self.key = nn.Linear(nEmb, headSiz, bias=False)\n",
        "        self.quary = nn.Linear(nEmb, headSiz, bias=False)\n",
        "        self.value = nn.Linear(nEmb, headSiz, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(blockSiz, blockSiz)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "\n",
        "        k = self.key(x)\n",
        "        q = self.quary(x)\n",
        "\n",
        "        w = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5\n",
        "        w = w.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        w = F.softmax(w, dim=-1)\n",
        "        w = self.dropout(w)\n",
        "\n",
        "        v = self.value(x)\n",
        "        out = w @ v\n",
        "\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, nHead, headSiz):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.heads = nn.ModuleList([Head(headSiz) for i in range(nHead)])\n",
        "        self.projection = nn.Linear(headSiz * nHead, nEmb)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.projection(out))\n",
        "\n",
        "        return out\n",
        "\n",
        "class FeedForwardNetowrk(nn.Module):\n",
        "    def __init__(self, nEmb):\n",
        "        super(FeedForwardNetowrk, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            # 4*nEmb:\n",
        "            #  ,In atten Paper, inner lyr of the FFN, Should mul by 4 in terms of channle siz\n",
        "            nn.Linear(nEmb, 4 * nEmb),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * nEmb, nEmb),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, nEmb, nHead):\n",
        "        super(Block, self).__init__()\n",
        "        headSiz = nEmb // nHead\n",
        "        self.selfAttn = MultiHeadAttention(nHead, headSiz)\n",
        "        self.ffn = FeedForwardNetowrk(nEmb)\n",
        "        self.layerNorm_1 = nn.LayerNorm(nEmb)\n",
        "        self.layerNorm_2 = nn.LayerNorm(nEmb)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.selfAttn(self.layerNorm_1(x))\n",
        "        x = x + self.ffn(self.layerNorm_2(x))\n",
        "\n",
        "        return x\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BigramLanguageModel, self).__init__()\n",
        "        self.toknEmbTable = nn.Embedding(vocabSiz, nEmb)\n",
        "        self.posEmbTable = nn.Embedding(blockSiz, nEmb)\n",
        "        self.blocks = nn.Sequential(*[Block(nEmb, nHead=nHead) for i in range(nLayers)])\n",
        "        self.finlLayrNorm = nn.LayerNorm(nEmb)\n",
        "        self.lmHead = nn.Linear(nEmb, vocabSiz)\n",
        "\n",
        "        self.apply(self._init__weights)\n",
        "\n",
        "    def _init__weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, ix, targt=None):\n",
        "        B, T = ix.shape\n",
        "\n",
        "        tokEmb = self.toknEmbTable(ix)\n",
        "        posEmb = self.posEmbTable(torch.arange(T, device=device))\n",
        "        x = tokEmb + posEmb\n",
        "        x = self.blocks(x)\n",
        "        x = self.finlLayrNorm(x)\n",
        "        logits = self.lmHead(x)\n",
        "\n",
        "        if targt is None:\n",
        "            loss = None\n",
        "\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targt = targt.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targt)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def genarate(self, ix, maxNewTokn):\n",
        "        for i in range(maxNewTokn):\n",
        "            ixCond = ix[:, -blockSiz:]\n",
        "            logits, loss = self(ixCond)\n",
        "\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "            ixNxt = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "            ix = torch.cat((ix, ixNxt), dim=1)\n",
        "\n",
        "        return ix\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device=device)\n"
      ],
      "metadata": {
        "id": "IchHFCtWbJQ0"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(sum(p.numel() for p in m.parameters()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DimFT3DbMvC",
        "outputId": "719a1a22-73ed-4140-b732-e16630956cfa"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "974513\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "\n",
        "for i in range(epochs):\n",
        "\n",
        "    if i % evalInterval == 0 or i == epochs-1:\n",
        "        losses = estimateLoss()\n",
        "        print(f\"step {i}, trainLoss: [ {losses['train']:.4f} ], devLoss[ {losses['dev']:.4f} ]\")\n",
        "\n",
        "    xb, yb = getBatch('train')\n",
        "\n",
        "    logits, loss = model(xb, yb)\n",
        "    optim.zero_grad()\n",
        "    loss.backward()\n",
        "    optim.step()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddaKwudbbOcz",
        "outputId": "7234638f-8798-4d62-a454-1054ab89565e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0, trainLoss: [ 4.7391 ], devLoss[ 4.7419 ]\n",
            "step 500, trainLoss: [ 2.8791 ], devLoss[ 2.8649 ]\n",
            "step 1000, trainLoss: [ 2.8544 ], devLoss[ 2.8361 ]\n",
            "step 1500, trainLoss: [ 2.8205 ], devLoss[ 2.7946 ]\n",
            "step 2000, trainLoss: [ 2.7821 ], devLoss[ 2.7541 ]\n",
            "step 2500, trainLoss: [ 2.7809 ], devLoss[ 2.7523 ]\n",
            "step 3000, trainLoss: [ 2.7626 ], devLoss[ 2.7406 ]\n",
            "step 3500, trainLoss: [ 2.8449 ], devLoss[ 2.8189 ]\n",
            "step 3999, trainLoss: [ 2.7913 ], devLoss[ 2.7595 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving Model\n"
      ],
      "metadata": {
        "id": "7QowNcDnjzCL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model's state dictionary\n",
        "modelSavePath = 'trained_BigramLM_Para.pth'\n",
        "torch.save(model.state_dict(), modelSavePath)\n"
      ],
      "metadata": {
        "id": "g70fD2s-jatw"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the entire model\n",
        "fullModelPath = \"bigram_LM_full.pth\"\n",
        "torch.save(model, fullModelPath)\n",
        "print(f\"Entire model saved to {fullModelPath}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUDcnRH0kmYl",
        "outputId": "27edf901-b02a-4db4-fdb7-d236c1b783ee"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entire model saved to bigram_LM_full.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sadly i got runOutOF GPU Usage on Google Colab On training,\n",
        "# Its taking Way too fucking Long\n",
        "\n",
        "# So i descaled the model. and reTrained,, it got like 50m, before with scaled model, it got 1h,12. when i stop it,\n",
        "\n",
        "# However The Sampling are Trash"
      ],
      "metadata": {
        "id": "SSs27CfKq4HX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "contxt = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decod(m.genarate(contxt, maxNewTokn=1000)[0].tolist()))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kFGgcAcjbQmi",
        "outputId": "b3ac6304-8a3f-42d4-8076-a41ce19fbd1a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "etou avs e  ns tom” a. nineydhe wen\n",
            "l hzü  weebvei o r t hia mant o5 shos?trdr otapar s.oricyskwittrHerhurivitHtwhepitrccem em oercstanrtt nnsss Iswanrapvat\n",
            "etty gthe tif ersoisiohpirsI pswtàimofrotaas  rhboc aue f a, suinoa poanreinenn w aitavmh  rystisincaslrn. Totaslstmh In cneem   thtbelrri e wtbul bbtnxoendptmw wrou hl,the tIon sincoum n inutdtconaseericyted oni, oegjs toanm pudalss ofsolt hext u hrufs Imod hiifxpm\n",
            "rerhoos terweso tnarU upd.ise warwFqtsplgolrvootOralc-c biorioru,os ee s dn\n",
            "fn nmaitwf i2 iu svesotu bmrtdoyte tb   podtse ta) lysspt t rFlld sd h eceistfrcnrttom\n",
            "rtihe tan pooc oeefdsen tis t y a w  s\n",
            "owpma mtsocehin n urg ic rgeowcelal hon f e a b ilf rm o Tiacr  toas rweorbanmrm rs blnrf\n",
            "oerunsnrbttrcefdpf\n",
            "tnm ocoiichooe\n",
            "tTnse ud ousse rmtifesssgseneslanreb ItLddu  tIptf ramh wh  sor gsccmpr rnnontftyu\n",
            "wH;ve aeicicis oielits wsan \n",
            "am \n",
            "lbt p tpco s, th whots[n dsiptpr ttamot sh rabootT\n",
            " sann,ds an, as\n",
            "acoapu fe. thheasungsea\n",
            "pptu  orpoerdtb“ ofalooopjnelrsh, ou; weha \n"
          ]
        }
      ]
    }
  ]
}