{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqIymehybro4"
      },
      "source": [
        "# NOTE:\n",
        " + Doc about the data file\n",
        " +  "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WARNING >> !!\n",
        "\n",
        " * Always test Run Before Train"
      ],
      "metadata": {
        "id": "1vPHiPieb2NY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ywv7xEq8brpA",
        "outputId": "88b451da-5d8a-45c1-f4c0-c6c4da9a4ce8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['\\n', ' ', '!', '\"', '&', \"'\", '(', ')', '*', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'á', 'é', 'ü', 'е', '\\u2005', '\\u200b', '‘', '’', '\\u205f']\n",
            "89\n"
          ]
        }
      ],
      "source": [
        "with open('tiny_Ksi.txt', 'r', encoding='utf-8') as file:\n",
        "    txt = file.read()\n",
        "\n",
        "chars = sorted(list(set(txt)))\n",
        "vocabSiz = len(chars)\n",
        "\n",
        "print(chars)\n",
        "print(vocabSiz)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9rLsLiyebrpG",
        "outputId": "597719a0-83c8-4212-8e97-f7e289f881d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "155681\n"
          ]
        }
      ],
      "source": [
        "print(len(txt))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NsudJHPTbrpH",
        "outputId": "7ad74dae-50ec-4151-f7fb-8b44ca47bfd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step./ 0: Train Loss [4.5264], val/dev Loss [4.5216]\n",
            "Step./ 500: Train Loss [1.8160], val/dev Loss [1.9927]\n",
            "Step./ 1000: Train Loss [1.4977], val/dev Loss [1.7801]\n",
            "Step./ 1500: Train Loss [1.2876], val/dev Loss [1.7229]\n",
            "Step./ 2000: Train Loss [1.1254], val/dev Loss [1.7134]\n",
            "Step./ 2499: Train Loss [0.9754], val/dev Loss [1.7104]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# -----------\n",
        "# // Hyper para\n",
        "batchSiz = 44\n",
        "blockSiz = 168\n",
        "epochs = 3000\n",
        "evalIntervals = 500\n",
        "lr = 3e-1\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "evelItrs = 200\n",
        "nEmb = 256\n",
        "nHead = 5\n",
        "nLayers = 5\n",
        "dropout = 0.2\n",
        "# -----------\n",
        "\n",
        "\n",
        "# Test Run\n",
        "#batchSiz = 10\n",
        "#blockSiz = 25\n",
        "#epochs = 100\n",
        "#evalIntervals = 10\n",
        "#lr = 1e-1\n",
        "#device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "#evelItrs = 200\n",
        "#nEmb = 20\n",
        "#nHead = 1\n",
        "#nLayers = 1\n",
        "#dropout = 0.1\n",
        "#torch.manual_seed(1337)\n",
        "\n",
        "with open('tiny_Ksi.txt', 'r', encoding='utf-8') as file:\n",
        "    txt = file.read()\n",
        "\n",
        "chars = sorted(list(set(txt)))\n",
        "vocabSiz = len(chars)\n",
        "\n",
        "stoi = {ch:i for i, ch in enumerate(chars)}\n",
        "itos = {i:ch for i, ch in enumerate(chars)}\n",
        "enc = lambda s: [stoi[c] for c in s]\n",
        "decod = lambda l: \"\".join([itos[i] for i in l])\n",
        "\n",
        "data = torch.tensor(enc(txt), dtype=torch.long)\n",
        "n = int(0.9*len(data))\n",
        "trainData = data[:n]\n",
        "devData = data[n:]\n",
        "\n",
        "def getBatch(split):\n",
        "    data = trainData if split == 'train' else devData\n",
        "    ix = torch.randint(len(data) - blockSiz, (batchSiz, ))\n",
        "\n",
        "    x = torch.stack([data[i:i+blockSiz] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+blockSiz+1] for i in ix])#predict nxt token\n",
        "    x, y = x.to(device), y.to(device)\n",
        "\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimateLoss():\n",
        "    out = { }\n",
        "    model.eval()\n",
        "    for split in ['train', 'dev']:\n",
        "        losses = torch.zeros(evelItrs)\n",
        "\n",
        "        for k in range(evelItrs):\n",
        "            X, Y = getBatch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    def __init__(self, headSiz):\n",
        "        super(Head, self).__init__()\n",
        "        self.key = nn.Linear(nEmb, headSiz, bias=False)\n",
        "        self.quary = nn.Linear(nEmb, headSiz, bias=False)\n",
        "        self.value = nn.Linear(nEmb, headSiz, bias=False)\n",
        "\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(blockSiz, blockSiz)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #// input of size (batch, time-step, channels)\n",
        "        #// output of size (batch, time-step, head size)\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)\n",
        "        q = self.quary(x)\n",
        "\n",
        "        w = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5\n",
        "        w = w.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        w = F.softmax(w, dim=-1)\n",
        "        w = self.dropout(w)\n",
        "\n",
        "        v = self.value(x)\n",
        "        out = w @ v\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, nHead, headSiz):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.heads = nn.ModuleList([Head(headSiz) for _ in range(nHead)])\n",
        "        self.projection = nn.Linear(headSiz * nHead, nEmb)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.projection(out))\n",
        "        return out\n",
        "\n",
        "class FeedForwardNetwork(nn.Module):\n",
        "    def __init__(self, nEmb):\n",
        "        super(FeedForwardNetwork, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(nEmb, 4 * nEmb),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * nEmb, nEmb),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, nEmb, nHead):\n",
        "        super(Block, self).__init__()\n",
        "        headSiz = nEmb // nHead\n",
        "        self.selfAtn = MultiHeadAttention(nHead, headSiz)\n",
        "        self.ffn = FeedForwardNetwork(nEmb)\n",
        "        self.layrNorm_1 = nn.LayerNorm(nEmb)\n",
        "        self.layrNorm_2 = nn.LayerNorm(nEmb)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.selfAtn(self.layrNorm_1(x))\n",
        "        x = x + self.ffn(self.layrNorm_2(x))\n",
        "        return x\n",
        "\n",
        "class GPTLanguageModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GPTLanguageModel, self).__init__()\n",
        "        self.toknEmbTable = nn.Embedding(vocabSiz, nEmb)\n",
        "        self.posEmbTable = nn.Embedding(blockSiz, nEmb)\n",
        "        self.blocks = nn.Sequential(*[Block(nEmb, nHead=nHead) for _ in range(nLayers)])\n",
        "        self.lyrNormFinl = nn.LayerNorm(nEmb)\n",
        "        self.lmHead = nn.Linear(nEmb, vocabSiz)\n",
        "\n",
        "        self.apply(self._init_W)\n",
        "\n",
        "    def _init_W(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "\n",
        "    def forward(self, ix, targt=None):\n",
        "        B, T = ix.shape\n",
        "\n",
        "        tokEmb = self.toknEmbTable(ix)\n",
        "        posEmb = self.posEmbTable(torch.arange(T, device=device))\n",
        "        x = tokEmb + posEmb\n",
        "        x = self.blocks(x)\n",
        "        x = self.lyrNormFinl(x)\n",
        "        logits = self.lmHead(x)\n",
        "\n",
        "        if targt is None:\n",
        "            loss = None\n",
        "\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targt = targt.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targt)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, ix, maxNewTokn):\n",
        "        for i in range(maxNewTokn):\n",
        "            ixCond = ix[:, -blockSiz:]\n",
        "\n",
        "            logits, loss = self(ixCond)\n",
        "\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "            ixNxt = torch.multinomial(probs, num_samples=1)\n",
        "            ix = torch.cat((ix, ixNxt), dim=-1)\n",
        "        return ix\n",
        "\n",
        "model = GPTLanguageModel()\n",
        "m = model.to(device)\n",
        "\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "\n",
        "lossI = []\n",
        "for i in range(epochs):\n",
        "\n",
        "    if i % evalIntervals == 0 or i == epochs - 1:\n",
        "        losses = estimateLoss()\n",
        "        lossI.append(losses)\n",
        "\n",
        "        print(f\"Step./ {i}: Train Loss [{losses['train']:.4f}], val/dev Loss [{losses['dev']:.4f}]\")\n",
        "\n",
        "    xb, yb = getBatch('train')\n",
        "\n",
        "    logits, loss = model(xb, yb)\n",
        "    optim.zero_grad()\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "\n",
        "\n",
        "# Save model's state dictionary\n",
        "torch.save(model.state_dict(), \"KsiSongLyricsGEN_1.pth\")\n",
        "print(f'Model Save To KsiSongLyricsGEN_1.pth\"')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "contxt = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decod(m.generate(contxt, maxNewTokn=1000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0Rmpx8lgfrx",
        "outputId": "576c63aa-4bf3-4036-aa5c-e96031a23c50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " finght on the 62 were, alway ang out your get (Ayy, ayy)\n",
            "Whyle winning allazy\n",
            "My where these I said on me, (ah)\n",
            "Diving walone is piating to piecal? When then I ain't never your got\n",
            "I'm int when when ice, it's lifestmenty milessince\n",
            "Flions, I'm a rollatede killas aP\n",
            "KSI\n",
            "Had and gling my oh All\n",
            "I've bit got runng out their is rol\n",
            "See wher a the chillines\n",
            "We I'm sticsitabll me witchen shellybut I don't adver up cold\n",
            "Walkin' (Ayy, afpop)\n",
            "So in, I cant me to the luck up, fake up (Yeah)\n",
            "Andvay money're my bickaying drinks, I'm girldner, finin' Oh lost on (Ooh)\n",
            "I need poppin' me)\n",
            "Papin' ta never fuckin', the contem (Or trip)\n",
            "Sun, here, brom ain't no every men playin' no kin'\n",
            "This you missin', I don't game\n",
            "And kneh same\n",
            "No many gouess, Milline pleft boy Kepin' of the dark\n",
            "Back, I hen I wanna tin' make F's thance\n",
            "Everythhing that us on plead\n",
            "Acinging it (We me)\n",
            "Everything iving is on plents\n",
            "I'm sinconnelss ints when no my (Then I'm picking if)\n",
            "Reciding in, any creply, not and hampless, I'm con\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## WOW, Im Quite Happy, This is better that JJ's Lyrics :0\n",
        "  * Actualy Not bad for this model....\n",
        "\n",
        "    * About the data file:\n",
        "      * ok so i didnt Fucking webscrape the Lyrics,,\n",
        "      \n",
        "      * Cos Its So Foggy to figure out and I just copy Past\n",
        "\n",
        "      * But i think The model quite good,, Also the len(char) of txt == >150K\n",
        "  \n",
        "  * Also The tiny_Ksi.txt has Small Amout of data\n",
        "  \n",
        "### TRain Again with more DATA / Tiny_KSI_.txt\n"
      ],
      "metadata": {
        "id": "84IFllHEhqnr"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3ZH9YxJHjDN9"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}