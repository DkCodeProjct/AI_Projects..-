{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# minbpe exercise\n",
    "\n",
    "  At this point you have everything you need to build your own GPT-4 tokenizer. This is the exercise progression you may wish to follow. You'll note that it is part of the minbpe repo, which is the solution to that exercise, and is a cleaned up version of the code above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getStats(ids):\n",
    "    counts = {}\n",
    "    for pair in zip(ids, ids[1:]): #get consecutive ordr\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 99, 7, 9, 1]\n"
     ]
    }
   ],
   "source": [
    "def merge(ids, pair, ix):\n",
    "    newId = [ ]\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
    "            newId.append(ix)\n",
    "            i += 2\n",
    "        else:\n",
    "            newId.append(ids[i])\n",
    "            i += 1\n",
    "    return newId\n",
    "\n",
    "print(merge([5, 6, 6, 7, 9, 1], (6, 6), 99))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 :\n",
    "   + Write the BasicTokenizer class, with the following three core functions:\n",
    "\n",
    "      * def train(self, text, vocab_size, verbose=False)\n",
    "\n",
    "\n",
    "      * def encode(self, text)\n",
    "\n",
    "\n",
    "      * def decode(self, ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BasicTokenizer:\n",
    "    def __init__(self, token):\n",
    "        self.merges = {}\n",
    "        self.vocabSiz = 256\n",
    "        self.numMerges = self.vocabSiz - 256\n",
    "        self.token = token\n",
    "\n",
    "    def train(self, txt, vocabSiz, verbose=False):\n",
    "        vocabSiz = 256\n",
    "        ids = list(self.token)\n",
    "        for i in range(self.numMerges):\n",
    "            stats = getStats(ids)\n",
    "            pair = max(stats, key=stats.get)\n",
    "            ix = 256 + i\n",
    "            print(f'mergin para: {pair}, to --> {ix}')\n",
    "            ids = merge(ids, pair, ix)\n",
    "        return ids \n",
    "\n",
    "    def encode(self, txt):\n",
    "        token = list(txt.encode('utf-8'))\n",
    "        while len(token) >= 2:\n",
    "            stats = getStats(token)\n",
    "            ## if not pair that didnt emegi, assing it to inf\n",
    "            pair = min(stats, key=lambda p: self.merges.get(p, float('inf')))\n",
    "            if pair not in self.merges:\n",
    "                break\n",
    "            ix = self.merges[pair]\n",
    "            token = merge(token, pair, ix)\n",
    "        return token\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        # token id to byte Object For that token\n",
    "        vocab = {ix:bytes([ix]) for ix in range(256)} \n",
    "        for (p0, p1), ix in self.merges.items():\n",
    "            vocab[ix] = vocab[p0] + vocab[p1] \n",
    "\n",
    "        \n",
    "        tokens = b\"\".join(vocab[ix] for ix in ids)\n",
    "        txt = tokens.decode('utf-8', errors='replace')\n",
    "        return txt \n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Try, Got Help from minbpe Repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getStats(ids, counts=None):\n",
    "    counts = { } if counts is None else counts\n",
    "    # pythonic way of iterating consecutive element\n",
    "    for pair in zip(ids, ids[1:]):\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "\n",
    "    return counts  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(ids, pair, ix):\n",
    "    newId = [ ]\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
    "            newId.append(ix)\n",
    "            i += 2\n",
    "        else:\n",
    "            newId.append(ids[i])\n",
    "            i += 1\n",
    "    return newId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata \n",
    "\n",
    "# first two helper functions...// \n",
    "\n",
    "# #Code from minbpe Repo../\n",
    "def replaceCtrlChar(s: str) -> str:\n",
    "    chars = [ ]\n",
    "    for ch in s:\n",
    "        if unicodedata.category(ch)[0] != \"C\":\n",
    "            chars.append(ch) # this character is ok\n",
    "        \n",
    "        else:\n",
    "            chars.append(f\"\\\\u{ord(ch):04x}\") # escape\n",
    "    \n",
    "    return \"\".join(chars)\n",
    "\n",
    "def renderToken(t: bytes) -> str:\n",
    "    s = t.decode('utf-8', errors='replace')\n",
    "    s = replaceCtrlChar(s)\n",
    "\n",
    "    return s \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self):\n",
    "        self.merges = {}\n",
    "        self.pattern = \"\"\n",
    "        self.specialToken = {}\n",
    "        self.vocab = self._buildVocab()\n",
    "    \n",
    "    def train(self, txt, vocabSiz, verbose=False):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def enc(self, txt):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def decod(self, ids):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def _buildVocab(self):\n",
    "        vocab = {ix: bytes([ix]) for ix in range(256)}\n",
    "        for (p0, p1), ix in self.merges.items():\n",
    "            vocab[ix] = vocab[p0] + vocab[p1]\n",
    "        \n",
    "        for special, ix in self.specialToken.items():\n",
    "            vocab[ix] = special.encode('utf-8')\n",
    "        \n",
    "        return vocab\n",
    "\n",
    "    \n",
    "    def save(self, filPrefx):\n",
    "        modelFile = filPrefx + \".model\"\n",
    "        with open(modelFile, 'w') as file:\n",
    "            file.write(\"minbpe v1\\n\")\n",
    "            file.write(f\"{self.pattern}\\n\")\n",
    "\n",
    "            file.write(f\"{len(self.specialToken)}\\n\")\n",
    "            for special, ix in self.specialToken.items():\n",
    "                file.write(f\"{special}  {ix}\\n\")\n",
    "            \n",
    "            for ix1, ix2 in self.merges:\n",
    "                file.write(f\"{ix1} {ix2}\\n\")\n",
    "        \n",
    "        vocabSiz = filPrefx + '.vocab'\n",
    "        invertedMerges = {ix:pair for pair, ix in self.merges.items()}\n",
    "\n",
    "        with open(vocabSiz, 'w', encoding='utf-8') as file:\n",
    "            for ix, token in self.vocab.items():\n",
    "\n",
    "                s = renderToken(token)\n",
    "                # find the children of this token, if any\n",
    "                if ix in invertedMerges:\n",
    "                    ix0, ix1 = invertedMerges[ix]\n",
    "                    s0 = renderToken(self.vocab[ix0])\n",
    "                    s1 = renderToken(self.vocab[ix1])\n",
    "                    file.write(f\"[{s0}] [{s1}] -> [{s}] [{ix}]\\n\")\n",
    "                \n",
    "                else:\n",
    "                    file.write(f\"[{s}] {ix}\\n\")\n",
    "    \n",
    "\n",
    "    def load(self, modelFile):\n",
    "        assert modelFile.endswith(\".model\")\n",
    "\n",
    "        merges = {}\n",
    "        specialToken = {}\n",
    "        ix = 256\n",
    "\n",
    "        with open(modelFile, 'r', encoding='utf-8') as file:\n",
    "            vesrion = file.readline().strip()\n",
    "            assert vesrion == \"minbpe v1\"\n",
    "\n",
    "            self.patrn = file.readline().strip()\n",
    "\n",
    "            numSpecial = int(file.readline().strip().split())\n",
    "            \n",
    "            for i in range(numSpecial):\n",
    "                special, specailIx = file.readline().strip().split()\n",
    "                specialToken[special] = int(specailIx)\n",
    "            \n",
    "            for line in file:\n",
    "                ix1, ix2 = map(int, line.split())\n",
    "                merges[(ix1, ix2)] = ix \n",
    "                ix += 1\n",
    "        \n",
    "        self.merges = merges\n",
    "        self.specialToken = specialToken\n",
    "        self.vocab = self._buildVocab()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicTokenizer(Tokenizer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def train(self, txt, vocabSiz, verbose=False):\n",
    "        assert vocabSiz >= 256\n",
    "        numMerges = vocabSiz - 256\n",
    "\n",
    "        txtByts = txt.encode('utf-8')\n",
    "        ids = list(txtByts)\n",
    "\n",
    "        mergs = {}\n",
    "        vocab = {ix:bytes([ix]) for ix in range(256)}\n",
    "\n",
    "        for i in range(numMerges):\n",
    "            stats = getStats(ids)\n",
    "\n",
    "            pair = max(stats, key=stats.get)\n",
    "\n",
    "            ix = 256 + i\n",
    "\n",
    "            ids = merge(ids, pair, ix)\n",
    "\n",
    "            mergs[pair] = ix \n",
    "            vocab[ix] = vocab[pair[0]] + vocab[pair[1]]\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"merge {i+1}/{numMerges}: {pair} -> {ix} ({vocab[ix]}) has {stats[pair]} occurence\")\n",
    "        \n",
    "        self.merges = mergs\n",
    "        self.vocab = vocab\n",
    "    \n",
    "    def decod(self, ids):\n",
    "        txtByts = b\"\".join(self.vocab[ix] for ix in ids)\n",
    "        txt = txtByts.decode('utf-8', errors='replace')\n",
    "\n",
    "        return txt \n",
    "    \n",
    "\n",
    "    def enc(self, txt):\n",
    "        txtByts = txt.encode('utf-8')\n",
    "        ids = list(txtByts)\n",
    "        while len(ids) >= 2:\n",
    "            stats = getStats(ids)\n",
    "            pair = min(stats, key=lambda p: self.merges.get(p, float('inf')))\n",
    "\n",
    "            if pair not in self.merges:\n",
    "                break\n",
    "\n",
    "            ix = self.merges[pair]\n",
    "            ids = merge(ids, pair, ix)\n",
    "        \n",
    "        return ids "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merge 1/256: (101, 32) -> 256 (b'e ') has 2981 occurence\n",
      "merge 2/256: (44, 32) -> 257 (b', ') has 2961 occurence\n",
      "merge 3/256: (100, 32) -> 258 (b'd ') has 2617 occurence\n",
      "merge 4/256: (46, 32) -> 259 (b'. ') has 2560 occurence\n",
      "merge 5/256: (114, 32) -> 260 (b'r ') has 2428 occurence\n",
      "merge 6/256: (50, 48) -> 261 (b'20') has 2365 occurence\n",
      "merge 7/256: (115, 32) -> 262 (b's ') has 2053 occurence\n",
      "merge 8/256: (105, 110) -> 263 (b'in') has 2006 occurence\n",
      "merge 9/256: (111, 110) -> 264 (b'on') has 1815 occurence\n",
      "merge 10/256: (114, 105) -> 265 (b'ri') has 1805 occurence\n",
      "merge 11/256: (116, 32) -> 266 (b't ') has 1802 occurence\n",
      "merge 12/256: (116, 104) -> 267 (b'th') has 1737 occurence\n",
      "merge 13/256: (101, 258) -> 268 (b'ed ') has 1736 occurence\n",
      "merge 14/256: (257, 261) -> 269 (b', 20') has 1705 occurence\n",
      "merge 15/256: (97, 110) -> 270 (b'an') has 1487 occurence\n",
      "merge 16/256: (97, 114) -> 271 (b'ar') has 1360 occurence\n",
      "merge 17/256: (101, 260) -> 272 (b'er ') has 1356 occurence\n",
      "merge 18/256: (121, 32) -> 273 (b'y ') has 1248 occurence\n",
      "merge 19/256: (97, 108) -> 274 (b'al') has 1164 occurence\n",
      "merge 20/256: (267, 256) -> 275 (b'the ') has 1142 occurence\n",
      "merge 21/256: (118, 268) -> 276 (b'ved ') has 1104 occurence\n",
      "merge 22/256: (119, 105) -> 277 (b'wi') has 1049 occurence\n",
      "merge 23/256: (101, 114) -> 278 (b'er') has 897 occurence\n",
      "merge 24/256: (264, 32) -> 279 (b'on ') has 880 occurence\n",
      "merge 25/256: (277, 102) -> 280 (b'wif') has 871 occurence\n",
      "merge 26/256: (82, 101) -> 281 (b'Re') has 870 occurence\n",
      "merge 27/256: (83, 280) -> 282 (b'Swif') has 867 occurence\n",
      "merge 28/256: (111, 260) -> 283 (b'or ') has 859 occurence\n",
      "merge 29/256: (99, 104) -> 284 (b'ch') has 816 occurence\n",
      "merge 30/256: (269, 49) -> 285 (b', 201') has 811 occurence\n",
      "merge 31/256: (111, 109) -> 286 (b'om') has 789 occurence\n",
      "merge 32/256: (98, 272) -> 287 (b'ber ') has 752 occurence\n",
      "merge 33/256: (32, 275) -> 288 (b' the ') has 748 occurence\n",
      "merge 34/256: (97, 121) -> 289 (b'ay') has 744 occurence\n",
      "merge 35/256: (101, 110) -> 290 (b'en') has 740 occurence\n",
      "merge 36/256: (111, 114) -> 291 (b'or') has 737 occurence\n",
      "merge 37/256: (274, 32) -> 292 (b'al ') has 705 occurence\n",
      "merge 38/256: (101, 109) -> 293 (b'em') has 703 occurence\n",
      "merge 39/256: (46, 10) -> 294 (b'.\\n') has 685 occurence\n",
      "merge 40/256: (265, 101) -> 295 (b'rie') has 685 occurence\n",
      "merge 41/256: (263, 103) -> 296 (b'ing') has 684 occurence\n",
      "merge 42/256: (269, 50) -> 297 (b', 202') has 673 occurence\n",
      "merge 43/256: (116, 105) -> 298 (b'ti') has 666 occurence\n",
      "merge 44/256: (289, 108) -> 299 (b'ayl') has 654 occurence\n",
      "merge 45/256: (34, 259) -> 300 (b'\". ') has 651 occurence\n",
      "merge 46/256: (108, 108) -> 301 (b'll') has 649 occurence\n",
      "merge 47/256: (84, 299) -> 302 (b'Tayl') has 647 occurence\n",
      "merge 48/256: (116, 295) -> 303 (b'trie') has 644 occurence\n",
      "merge 49/256: (294, 32) -> 304 (b'.\\n ') has 643 occurence\n",
      "merge 50/256: (116, 111) -> 305 (b'to') has 642 occurence\n",
      "merge 51/256: (259, 281) -> 306 (b'. Re') has 640 occurence\n",
      "merge 52/256: (306, 303) -> 307 (b'. Retrie') has 639 occurence\n",
      "merge 53/256: (307, 276) -> 308 (b'. Retrieved ') has 639 occurence\n",
      "merge 54/256: (302, 283) -> 309 (b'Taylor ') has 611 occurence\n",
      "merge 55/256: (101, 115) -> 310 (b'es') has 606 occurence\n",
      "merge 56/256: (309, 282) -> 311 (b'Taylor Swif') has 598 occurence\n",
      "merge 57/256: (117, 115) -> 312 (b'us') has 561 occurence\n",
      "merge 58/256: (114, 286) -> 313 (b'rom') has 532 occurence\n",
      "merge 59/256: (293, 287) -> 314 (b'ember ') has 528 occurence\n",
      "merge 60/256: (41, 259) -> 315 (b'). ') has 524 occurence\n",
      "merge 61/256: (65, 114) -> 316 (b'Ar') has 509 occurence\n",
      "merge 62/256: (102, 313) -> 317 (b'from') has 503 occurence\n",
      "merge 63/256: (315, 34) -> 318 (b'). \"') has 499 occurence\n",
      "merge 64/256: (270, 258) -> 319 (b'and ') has 498 occurence\n",
      "merge 65/256: (114, 101) -> 320 (b're') has 495 occurence\n",
      "merge 66/256: (111, 117) -> 321 (b'ou') has 487 occurence\n",
      "merge 67/256: (111, 265) -> 322 (b'ori') has 469 occurence\n",
      "merge 68/256: (111, 102) -> 323 (b'of') has 466 occurence\n",
      "merge 69/256: (103, 263) -> 324 (b'gin') has 465 occurence\n",
      "merge 70/256: (296, 32) -> 325 (b'ing ') has 464 occurence\n",
      "merge 71/256: (284, 105) -> 326 (b'chi') has 458 occurence\n",
      "merge 72/256: (93, 32) -> 327 (b'] ') has 458 occurence\n",
      "merge 73/256: (324, 292) -> 328 (b'ginal ') has 453 occurence\n",
      "merge 74/256: (317, 288) -> 329 (b'from the ') has 447 occurence\n",
      "merge 75/256: (322, 328) -> 330 (b'original ') has 446 occurence\n",
      "merge 76/256: (104, 256) -> 331 (b'he ') has 440 occurence\n",
      "merge 77/256: (316, 326) -> 332 (b'Archi') has 440 occurence\n",
      "merge 78/256: (332, 276) -> 333 (b'Archived ') has 440 occurence\n",
      "merge 79/256: (329, 330) -> 334 (b'from the original ') has 440 occurence\n",
      "merge 80/256: (333, 334) -> 335 (b'Archived from the original ') has 439 occurence\n",
      "merge 81/256: (335, 279) -> 336 (b'Archived from the original on ') has 438 occurence\n",
      "merge 82/256: (259, 336) -> 337 (b'. Archived from the original on ') has 433 occurence\n",
      "merge 83/256: (97, 32) -> 338 (b'a ') has 420 occurence\n",
      "merge 84/256: (115, 116) -> 339 (b'st') has 409 occurence\n",
      "merge 85/256: (105, 99) -> 340 (b'ic') has 406 occurence\n",
      "merge 86/256: (46, 91) -> 341 (b'.[') has 381 occurence\n",
      "merge 87/256: (101, 99) -> 342 (b'ec') has 374 occurence\n",
      "merge 88/256: (105, 301) -> 343 (b'ill') has 367 occurence\n",
      "merge 89/256: (39, 262) -> 344 (b\"'s \") has 367 occurence\n",
      "merge 90/256: (311, 266) -> 345 (b'Taylor Swift ') has 352 occurence\n",
      "merge 91/256: (111, 118) -> 346 (b'ov') has 343 occurence\n",
      "merge 92/256: (97, 116) -> 347 (b'at') has 334 occurence\n",
      "merge 93/256: (97, 262) -> 348 (b'as ') has 315 occurence\n",
      "merge 94/256: (101, 262) -> 349 (b'es ') has 309 occurence\n",
      "merge 95/256: (74, 117) -> 350 (b'Ju') has 307 occurence\n",
      "merge 96/256: (323, 32) -> 351 (b'of ') has 306 occurence\n",
      "merge 97/256: (305, 32) -> 352 (b'to ') has 284 occurence\n",
      "merge 98/256: (117, 109) -> 353 (b'um') has 281 occurence\n",
      "merge 99/256: (84, 331) -> 354 (b'The ') has 277 occurence\n",
      "merge 100/256: (271, 100) -> 355 (b'ard') has 277 occurence\n",
      "merge 101/256: (263, 32) -> 356 (b'in ') has 276 occurence\n",
      "merge 102/256: (270, 32) -> 357 (b'an ') has 276 occurence\n",
      "merge 103/256: (101, 108) -> 358 (b'el') has 275 occurence\n",
      "merge 104/256: (297, 51) -> 359 (b', 2023') has 271 occurence\n",
      "merge 105/256: (271, 273) -> 360 (b'ary ') has 259 occurence\n",
      "merge 106/256: (267, 32) -> 361 (b'th ') has 258 occurence\n",
      "merge 107/256: (97, 109) -> 362 (b'am') has 257 occurence\n",
      "merge 108/256: (108, 273) -> 363 (b'ly ') has 250 occurence\n",
      "merge 109/256: (111, 112) -> 364 (b'op') has 249 occurence\n",
      "merge 110/256: (311, 116) -> 365 (b'Taylor Swift') has 246 occurence\n",
      "merge 111/256: (116, 114) -> 366 (b'tr') has 243 occurence\n",
      "merge 112/256: (105, 115) -> 367 (b'is') has 234 occurence\n",
      "merge 113/256: (104, 272) -> 368 (b'her ') has 232 occurence\n",
      "merge 114/256: (111, 32) -> 369 (b'o ') has 225 occurence\n",
      "merge 115/256: (117, 360) -> 370 (b'uary ') has 225 occurence\n",
      "merge 116/256: (78, 346) -> 371 (b'Nov') has 222 occurence\n",
      "merge 117/256: (312, 340) -> 372 (b'usic') has 221 occurence\n",
      "merge 118/256: (371, 314) -> 373 (b'November ') has 221 occurence\n",
      "merge 119/256: (101, 119) -> 374 (b'ew') has 219 occurence\n",
      "merge 120/256: (97, 266) -> 375 (b'at ') has 219 occurence\n",
      "merge 121/256: (108, 32) -> 376 (b'l ') has 218 occurence\n",
      "merge 122/256: (58, 32) -> 377 (b': ') has 213 occurence\n",
      "merge 123/256: (98, 111) -> 378 (b'bo') has 210 occurence\n",
      "merge 124/256: (282, 266) -> 379 (b'Swift ') has 208 occurence\n",
      "merge 125/256: (68, 342) -> 380 (b'Dec') has 207 occurence\n",
      "merge 126/256: (105, 116) -> 381 (b'it') has 206 occurence\n",
      "merge 127/256: (105, 103) -> 382 (b'ig') has 205 occurence\n",
      "merge 128/256: (66, 343) -> 383 (b'Bill') has 205 occurence\n",
      "merge 129/256: (49, 48) -> 384 (b'10') has 204 occurence\n",
      "merge 130/256: (97, 115) -> 385 (b'as') has 203 occurence\n",
      "merge 131/256: (264, 103) -> 386 (b'ong') has 202 occurence\n",
      "merge 132/256: (79, 99) -> 387 (b'Oc') has 200 occurence\n",
      "merge 133/256: (97, 298) -> 388 (b'ati') has 199 occurence\n",
      "merge 134/256: (83, 116) -> 389 (b'St') has 198 occurence\n",
      "merge 135/256: (387, 305) -> 390 (b'Octo') has 198 occurence\n",
      "merge 136/256: (390, 287) -> 391 (b'October ') has 198 occurence\n",
      "merge 137/256: (97, 99) -> 392 (b'ac') has 197 occurence\n",
      "merge 138/256: (111, 119) -> 393 (b'ow') has 196 occurence\n",
      "merge 139/256: (380, 314) -> 394 (b'December ') has 194 occurence\n",
      "merge 140/256: (383, 378) -> 395 (b'Billbo') has 191 occurence\n",
      "merge 141/256: (97, 100) -> 396 (b'ad') has 190 occurence\n",
      "merge 142/256: (108, 101) -> 397 (b'le') has 190 occurence\n",
      "merge 143/256: (117, 114) -> 398 (b'ur') has 188 occurence\n",
      "merge 144/256: (102, 283) -> 399 (b'for ') has 188 occurence\n",
      "merge 145/256: (32, 40) -> 400 (b' (') has 187 occurence\n",
      "merge 146/256: (297, 50) -> 401 (b', 2022') has 187 occurence\n",
      "merge 147/256: (117, 103) -> 402 (b'ug') has 185 occurence\n",
      "merge 148/256: (284, 32) -> 403 (b'ch ') has 184 occurence\n",
      "merge 149/256: (115, 266) -> 404 (b'st ') has 181 occurence\n",
      "merge 150/256: (321, 110) -> 405 (b'oun') has 176 occurence\n",
      "merge 151/256: (98, 353) -> 406 (b'bum') has 172 occurence\n",
      "merge 152/256: (111, 108) -> 407 (b'ol') has 171 occurence\n",
      "merge 153/256: (312, 266) -> 408 (b'ust ') has 171 occurence\n",
      "merge 154/256: (101, 98) -> 409 (b'eb') has 170 occurence\n",
      "merge 155/256: (77, 97) -> 410 (b'Ma') has 170 occurence\n",
      "merge 156/256: (350, 363) -> 411 (b'July ') has 170 occurence\n",
      "merge 157/256: (318, 345) -> 412 (b'). \"Taylor Swift ') has 169 occurence\n",
      "merge 158/256: (107, 32) -> 413 (b'k ') has 165 occurence\n",
      "merge 159/256: (278, 115) -> 414 (b'ers') has 164 occurence\n",
      "merge 160/256: (93, 91) -> 415 (b'][') has 164 occurence\n",
      "merge 161/256: (65, 402) -> 416 (b'Aug') has 164 occurence\n",
      "merge 162/256: (416, 408) -> 417 (b'August ') has 163 occurence\n",
      "merge 163/256: (105, 100) -> 418 (b'id') has 161 occurence\n",
      "merge 164/256: (297, 49) -> 419 (b', 2021') has 160 occurence\n",
      "merge 165/256: (109, 101) -> 420 (b'me') has 159 occurence\n",
      "merge 166/256: (101, 112) -> 421 (b'ep') has 156 occurence\n",
      "merge 167/256: (261, 49) -> 422 (b'201') has 149 occurence\n",
      "merge 168/256: (50, 51) -> 423 (b'23') has 145 occurence\n",
      "merge 169/256: (285, 50) -> 424 (b', 2012') has 144 occurence\n",
      "merge 170/256: (101, 271) -> 425 (b'ear') has 140 occurence\n",
      "merge 171/256: (269, 261) -> 426 (b', 2020') has 140 occurence\n",
      "merge 172/256: (73, 110) -> 427 (b'In') has 139 occurence\n",
      "merge 173/256: (102, 105) -> 428 (b'fi') has 139 occurence\n",
      "merge 174/256: (110, 256) -> 429 (b'ne ') has 139 occurence\n",
      "merge 175/256: (395, 355) -> 430 (b'Billboard') has 136 occurence\n",
      "merge 176/256: (265, 116) -> 431 (b'rit') has 134 occurence\n",
      "merge 177/256: (104, 105) -> 432 (b'hi') has 133 occurence\n",
      "merge 178/256: (372, 32) -> 433 (b'usic ') has 133 occurence\n",
      "merge 179/256: (304, 34) -> 434 (b'.\\n \"') has 133 occurence\n",
      "merge 180/256: (78, 374) -> 435 (b'New') has 131 occurence\n",
      "merge 181/256: (100, 105) -> 436 (b'di') has 130 occurence\n",
      "merge 182/256: (65, 112) -> 437 (b'Ap') has 130 occurence\n",
      "merge 183/256: (285, 57) -> 438 (b', 2019') has 129 occurence\n",
      "merge 184/256: (114, 111) -> 439 (b'ro') has 128 occurence\n",
      "merge 185/256: (39, 32) -> 440 (b\"' \") has 128 occurence\n",
      "merge 186/256: (115, 257) -> 441 (b's, ') has 127 occurence\n",
      "merge 187/256: (350, 429) -> 442 (b'June ') has 127 occurence\n",
      "merge 188/256: (323, 288) -> 443 (b'of the ') has 126 occurence\n",
      "merge 189/256: (99, 291) -> 444 (b'cor') has 126 occurence\n",
      "merge 190/256: (50, 49) -> 445 (b'21') has 126 occurence\n",
      "merge 191/256: (49, 57) -> 446 (b'19') has 124 occurence\n",
      "merge 192/256: (105, 109) -> 447 (b'im') has 123 occurence\n",
      "merge 193/256: (290, 32) -> 448 (b'en ') has 123 occurence\n",
      "merge 194/256: (409, 114) -> 449 (b'ebr') has 122 occurence\n",
      "merge 195/256: (290, 116) -> 450 (b'ent') has 121 occurence\n",
      "merge 196/256: (111, 301) -> 451 (b'oll') has 121 occurence\n",
      "merge 197/256: (77, 271) -> 452 (b'Mar') has 120 occurence\n",
      "merge 198/256: (265, 99) -> 453 (b'ric') has 120 occurence\n",
      "merge 199/256: (277, 361) -> 454 (b'with ') has 120 occurence\n",
      "merge 200/256: (44, 91) -> 455 (b',[') has 118 occurence\n",
      "merge 201/256: (70, 449) -> 456 (b'Febr') has 118 occurence\n",
      "merge 202/256: (456, 370) -> 457 (b'February ') has 118 occurence\n",
      "merge 203/256: (365, 344) -> 458 (b\"Taylor Swift's \") has 118 occurence\n",
      "merge 204/256: (300, 430) -> 459 (b'\". Billboard') has 118 occurence\n",
      "merge 205/256: (101, 97) -> 460 (b'ea') has 116 occurence\n",
      "merge 206/256: (285, 54) -> 461 (b', 2016') has 116 occurence\n",
      "merge 207/256: (421, 116) -> 462 (b'ept') has 115 occurence\n",
      "merge 208/256: (410, 273) -> 463 (b'May ') has 115 occurence\n",
      "merge 209/256: (285, 53) -> 464 (b', 2015') has 115 occurence\n",
      "merge 210/256: (437, 265) -> 465 (b'Apri') has 115 occurence\n",
      "merge 211/256: (465, 376) -> 466 (b'April ') has 115 occurence\n",
      "merge 212/256: (108, 256) -> 467 (b'le ') has 113 occurence\n",
      "merge 213/256: (65, 119) -> 468 (b'Aw') has 112 occurence\n",
      "merge 214/256: (388, 264) -> 469 (b'ation') has 112 occurence\n",
      "merge 215/256: (83, 462) -> 470 (b'Sept') has 112 occurence\n",
      "merge 216/256: (470, 314) -> 471 (b'September ') has 112 occurence\n",
      "merge 217/256: (114, 97) -> 472 (b'ra') has 111 occurence\n",
      "merge 218/256: (274, 406) -> 473 (b'album') has 111 occurence\n",
      "merge 219/256: (67, 104) -> 474 (b'Ch') has 110 occurence\n",
      "merge 220/256: (118, 256) -> 475 (b've ') has 109 occurence\n",
      "merge 221/256: (310, 266) -> 476 (b'est ') has 108 occurence\n",
      "merge 222/256: (74, 270) -> 477 (b'Jan') has 108 occurence\n",
      "merge 223/256: (50, 50) -> 478 (b'22') has 107 occurence\n",
      "merge 224/256: (477, 370) -> 479 (b'January ') has 107 occurence\n",
      "merge 225/256: (405, 366) -> 480 (b'ountr') has 106 occurence\n",
      "merge 226/256: (382, 104) -> 481 (b'igh') has 106 occurence\n",
      "merge 227/256: (300, 354) -> 482 (b'\". The ') has 106 occurence\n",
      "merge 228/256: (359, 304) -> 483 (b', 2023.\\n ') has 106 occurence\n",
      "merge 229/256: (49, 51) -> 484 (b'13') has 105 occurence\n",
      "merge 230/256: (65, 108) -> 485 (b'Al') has 105 occurence\n",
      "merge 231/256: (101, 116) -> 486 (b'et') has 105 occurence\n",
      "merge 232/256: (310, 115) -> 487 (b'ess') has 103 occurence\n",
      "merge 233/256: (452, 403) -> 488 (b'March ') has 103 occurence\n",
      "merge 234/256: (117, 116) -> 489 (b'ut') has 102 occurence\n",
      "merge 235/256: (119, 431) -> 490 (b'writ') has 101 occurence\n",
      "merge 236/256: (108, 111) -> 491 (b'lo') has 99 occurence\n",
      "merge 237/256: (115, 386) -> 492 (b'song') has 97 occurence\n",
      "merge 238/256: (226, 128) -> 493 (b'\\xe2\\x80') has 97 occurence\n",
      "merge 239/256: (271, 258) -> 494 (b'ard ') has 97 occurence\n",
      "merge 240/256: (48, 32) -> 495 (b'0 ') has 97 occurence\n",
      "merge 241/256: (117, 108) -> 496 (b'ul') has 96 occurence\n",
      "merge 242/256: (50, 52) -> 497 (b'24') has 95 occurence\n",
      "merge 243/256: (105, 262) -> 498 (b'is ') has 94 occurence\n",
      "merge 244/256: (298, 99) -> 499 (b'tic') has 93 occurence\n",
      "merge 245/256: (97, 103) -> 500 (b'ag') has 93 occurence\n",
      "merge 246/256: (34, 32) -> 501 (b'\" ') has 93 occurence\n",
      "merge 247/256: (65, 110) -> 502 (b'An') has 93 occurence\n",
      "merge 248/256: (49, 56) -> 503 (b'18') has 93 occurence\n",
      "merge 249/256: (102, 291) -> 504 (b'for') has 90 occurence\n",
      "merge 250/256: (480, 273) -> 505 (b'ountry ') has 89 occurence\n",
      "merge 251/256: (65, 420) -> 506 (b'Ame') has 88 occurence\n",
      "merge 252/256: (506, 453) -> 507 (b'Americ') has 88 occurence\n",
      "merge 253/256: (32, 84) -> 508 (b' T') has 88 occurence\n",
      "merge 254/256: (115, 296) -> 509 (b'sing') has 87 occurence\n",
      "merge 255/256: (119, 348) -> 510 (b'was ') has 86 occurence\n",
      "merge 256/256: (49, 50) -> 511 (b'12') has 86 occurence\n",
      "Training took 13.41 seconds\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import time\n",
    "\n",
    "txtByts = open('taylorswift.txt', \"r\", encoding='utf-8').read()\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "tokenizr = BasicTokenizer()\n",
    "tokenizr.train(txtByts, 512, verbose=True)\n",
    "\n",
    "prefx = os.path.join('models', 'basic')\n",
    "tokenizr.save(prefx)\n",
    "\n",
    "t1 = time.time()\n",
    "print(f\"Training took {t1 - t0:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT2_SPLIT_PATTERN = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "GPT4_SPLIT_PATTERN =  r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegExTokenizer(Tokenizer):\n",
    "    def __init__(self, pattern=None):\n",
    "        super.__init__()\n",
    "        self.pattern = GPT4_SPLIT_PATTERN if pattern is None else pattern\n",
    "        self.compilPattrn = re.compile(self.pattern)\n",
    "        self.specialTokn = {}\n",
    "        self.inverseSpecialTokn = { }\n",
    "\n",
    "    def train(self, txt, vocabSiz, verbose=False):\n",
    "        assert vocabSiz >= 256\n",
    "        numMerges = vocabSiz - 256\n",
    "\n",
    "        txtChunks = re.findall(self.compilPattrn, txt)\n",
    "        ids = [list(ch.encode('utf-8')) for ch in txtChunks]\n",
    "\n",
    "\n",
    "        mergs = {}\n",
    "        vocab = {ix:bytes([ix]) for ix in range(256)}\n",
    "\n",
    "        for i in range(numMerges):\n",
    "            stats = { }\n",
    "            \n",
    "            for chunkIds in ids:\n",
    "                getStats(chunkIds, stats)\n",
    "\n",
    "            pair = max(stats, key=stats.get)\n",
    "\n",
    "            ix = 256 + i\n",
    "\n",
    "            ids = [merge(chnkIds, pair, ix) for chnkIds in ids]\n",
    "            \n",
    "\n",
    "            mergs[pair] = ix \n",
    "            vocab[ix] = vocab[pair[0]] + vocab[pair[1]]\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"merge {i+1}/{numMerges}: {pair} -> {ix} ({vocab[ix]}) has {stats[pair]} occurence\")\n",
    "        \n",
    "        self.merges = mergs\n",
    "        self.vocab = vocab\n",
    "\n",
    "    \n",
    "    def registrSpecialTokn(self, specialTokn):\n",
    "        self.specialTokn = specialTokn\n",
    "        self.inverseSpecialTokn = {v:k for k, v in specialTokn.items()}\n",
    "    \n",
    "    def decod(self, ids):\n",
    "        partByts = []\n",
    "        for ix in ids:\n",
    "            if ix in self.vocab:\n",
    "                partByts.append(self.vocab[ix])\n",
    "            \n",
    "            elif ix in self.inverseSpecialTokn:\n",
    "                partByts.append(self.inverseSpecialTokn[ix].encode('utf-8'))\n",
    "            \n",
    "            else:\n",
    "                raise ValueError(f'invalid token id {ix}')\n",
    "            \n",
    "        txtByts = b\"\".join(partByts)\n",
    "        txt = txtByts.decode('utf-8', errors='replace')\n",
    "\n",
    "        return txt \n",
    "    \n",
    "\n",
    "    def _encodeChunk(self, txtByts):\n",
    "\n",
    "        ids = list(txtByts)\n",
    "        while len(ids) >= 2:\n",
    "            stats = getStats(ids)\n",
    "            pair = min(stats, key=lambda p: self.merges.get(p, float('inf')))\n",
    "\n",
    "            if pair not in self.merges:\n",
    "                break\n",
    "\n",
    "            ix = self.merges[pair]\n",
    "            ids = merge(ids, pair, ix)\n",
    "        \n",
    "        return ids \n",
    "\n",
    "    def encodeOrdinary(self, txt):\n",
    "        \"\"\"Encoding that ignores any special tokens.\"\"\"\n",
    "        \n",
    "        txtChunks = re.findall(self.compilPattrn, txt)\n",
    "       \n",
    "        ids = []\n",
    "        for chunk in txtChunks:\n",
    "            chunkByts = chunk.encode(\"utf-8\") \n",
    "            chunkIds = self._encodeChunk(chunkByts) \n",
    "            ids.extend(chunkIds)\n",
    "        \n",
    "        return ids\n",
    "    \n",
    "\n",
    "    def enc(self, txt, allowdSpecial=\"none_raise\"):\n",
    "        special = None\n",
    "\n",
    "        if allowdSpecial == 'all':\n",
    "           special = self.specialToken\n",
    "        \n",
    "        elif allowdSpecial == 'none':\n",
    "            special = {}\n",
    "        \n",
    "        elif allowdSpecial == 'none_raise':\n",
    "            special = {}\n",
    "            assert all(tken not in txt for tken in self.specialToken)\n",
    "\n",
    "        elif isinstance(allowdSpecial, set):\n",
    "            special = {k:v for v, k in self.specialToken.items() if k in allowdSpecial} \n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f'allowd special {allowdSpecial} not understood')\n",
    "\n",
    "        if not special:\n",
    "            return self.encodeOrdinary(txt)\n",
    "        \n",
    "        specialPattrn = \"(\" + \"|\".join(re.escape(k) for k in special) + \")\"\n",
    "        specialChunk = re.split(specialPattrn, txt)\n",
    "\n",
    "        ids = []\n",
    "        for part in specialChunk:\n",
    "            if part in special:\n",
    "                ids.append(special[part])\n",
    "            \n",
    "            else:\n",
    "                ids.extend(self.encodeOrdinary(part))\n",
    "        \n",
    "        return ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3\n",
    "  + You're now ready to load the merges from the GPT-4 tokenizer and show that your tokenizer produces the identical results for both encode and decode, matching tiktoken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# match this\n",
    "import tiktoken\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\") # this is the GPT-4 tokenizer\n",
    "ids = enc.encode(\"hello world!!!? (안녕하세요!) lol123 😉\")\n",
    "text = enc.decode(ids) # get the same text back"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unfortunately, you will run into two issues:\n",
    "\n",
    " 1. It is not trivial to recover the raw merges from the GPT-4 tokenizer. You can easily recover what we call vocab here, and what they call and store under enc._mergeable_ranks. Feel free to copy paste the recover_merges function in minbpe/gpt4.py, which takes these ranks and returns the raw merges. If you wish to know how this function works, read this and this. Basically, under some conditions it is enough to only store the parent nodes (and their rank) and get rid of the precise details of which children merged up to any parent.\n",
    "\n",
    " \n",
    "  2. Second, the GPT-4 tokenizer for some reason permutes its raw bytes. It stores this permutation in the first 256 elements of the mergeable ranks, so you can recover this byte shuffle relatively simply as byte_shuffle = {i: enc._mergeable_ranks[bytes([i])] for i in range(256)}. In both your encode and decode, you'll have to shuffle bytes around accordingly. If you're stuck, reference the minbpe/gpt4.py` file for hints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\") # this is the GPT-4 tokenizer\n",
    "ids = enc.encode(\"<|endoftext|>hello world\", allowed_special=\"all\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
